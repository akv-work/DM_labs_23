{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJD-L1N_PAPx",
        "outputId": "335436d6-443f-4157-843b-b54ebd7a0c3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n",
            "\n",
            "==== Dataset: WDBC | N=569 F=30 ====\n",
            "Split sizes: 364 91 114\n",
            "\n",
            "--- Training from scratch ---\n",
            "Epoch 1/40 train_loss=0.7143 val_loss=0.7042 val_f1=0.0000\n",
            "Epoch 5/40 train_loss=0.5943 val_loss=0.5768 val_f1=0.9483\n",
            "Epoch 10/40 train_loss=0.1919 val_loss=0.1661 val_f1=0.9739\n",
            "Epoch 15/40 train_loss=0.0674 val_loss=0.0813 val_f1=0.9825\n",
            "Epoch 20/40 train_loss=0.0444 val_loss=0.0929 val_f1=0.9825\n",
            "Epoch 25/40 train_loss=0.0312 val_loss=0.0932 val_f1=0.9735\n",
            "Epoch 30/40 train_loss=0.0239 val_loss=0.0948 val_f1=0.9735\n",
            "Epoch 35/40 train_loss=0.0187 val_loss=0.0880 val_f1=0.9825\n",
            "Epoch 40/40 train_loss=0.0147 val_loss=0.0935 val_f1=0.9735\n",
            "Scratch eval: {'accuracy': 0.956140350877193, 'precision': 0.9855072463768116, 'recall': 0.9444444444444444, 'f1': 0.9645390070921985, 'roc_auc': np.float64(0.9943783068783069), 'confusion_matrix': array([[41,  1],\n",
            "       [ 4, 68]])}\n",
            "\n",
            "--- Layer-wise pretraining autoencoders ---\n",
            "\n",
            "Pretraining layer 1: 30 -> 64\n",
            "AE epoch 1/30 loss 1.099238\n",
            "AE epoch 10/30 loss 0.323581\n",
            "AE epoch 20/30 loss 0.144776\n",
            "AE epoch 30/30 loss 0.088569\n",
            "\n",
            "Pretraining layer 2: 64 -> 32\n",
            "AE epoch 1/30 loss 0.640508\n",
            "AE epoch 10/30 loss 0.195796\n",
            "AE epoch 20/30 loss 0.094007\n",
            "AE epoch 30/30 loss 0.061484\n",
            "\n",
            "Pretraining layer 3: 32 -> 16\n",
            "AE epoch 1/30 loss 1.224143\n",
            "AE epoch 10/30 loss 0.388969\n",
            "AE epoch 20/30 loss 0.222152\n",
            "AE epoch 30/30 loss 0.141597\n",
            "\n",
            "Pretraining layer 4: 16 -> 8\n",
            "AE epoch 1/30 loss 2.004041\n",
            "AE epoch 10/30 loss 1.760936\n",
            "AE epoch 20/30 loss 0.989153\n",
            "AE epoch 30/30 loss 0.447652\n",
            "\n",
            "--- Initializing MLP from pretrained encoders and finetune ---\n",
            "Epoch 1/40 train_loss=0.7672 val_loss=0.7600 val_f1=0.0000\n",
            "Epoch 5/40 train_loss=0.7373 val_loss=0.7299 val_f1=0.0000\n",
            "Epoch 10/40 train_loss=0.6997 val_loss=0.6986 val_f1=0.0000\n",
            "Epoch 15/40 train_loss=0.6711 val_loss=0.6691 val_f1=0.0000\n",
            "Epoch 20/40 train_loss=0.6383 val_loss=0.6383 val_f1=0.0345\n",
            "Epoch 25/40 train_loss=0.6058 val_loss=0.6054 val_f1=0.4384\n",
            "Epoch 30/40 train_loss=0.5705 val_loss=0.5690 val_f1=0.8000\n",
            "Epoch 35/40 train_loss=0.5259 val_loss=0.5272 val_f1=0.9259\n",
            "Epoch 40/40 train_loss=0.4829 val_loss=0.4799 val_f1=0.9464\n",
            "Pretrained eval: {'accuracy': 0.8596491228070176, 'precision': 1.0, 'recall': 0.7777777777777778, 'f1': 0.875, 'roc_auc': np.float64(0.9867724867724867), 'confusion_matrix': array([[42,  0],\n",
            "       [16, 56]])}\n",
            "\n",
            "==== Dataset: CreditApproval | N=690 F=15 ====\n",
            "Split sizes: 441 111 138\n",
            "\n",
            "--- Training from scratch ---\n",
            "Epoch 1/40 train_loss=0.6813 val_loss=0.6739 val_f1=0.0000\n",
            "Epoch 5/40 train_loss=0.6049 val_loss=0.5791 val_f1=0.7273\n",
            "Epoch 10/40 train_loss=0.3928 val_loss=0.3602 val_f1=0.8750\n",
            "Epoch 15/40 train_loss=0.3149 val_loss=0.2937 val_f1=0.8842\n",
            "Epoch 20/40 train_loss=0.2872 val_loss=0.2796 val_f1=0.8817\n",
            "Epoch 25/40 train_loss=0.2621 val_loss=0.2826 val_f1=0.8913\n",
            "Epoch 30/40 train_loss=0.2349 val_loss=0.2845 val_f1=0.9032\n",
            "Epoch 35/40 train_loss=0.2111 val_loss=0.2816 val_f1=0.8936\n",
            "Epoch 40/40 train_loss=0.1869 val_loss=0.2874 val_f1=0.8936\n",
            "Scratch eval: {'accuracy': 0.8623188405797102, 'precision': 0.85, 'recall': 0.8360655737704918, 'f1': 0.8429752066115702, 'roc_auc': np.float64(0.9416648924845646), 'confusion_matrix': array([[68,  9],\n",
            "       [10, 51]])}\n",
            "\n",
            "--- Layer-wise pretraining autoencoders ---\n",
            "\n",
            "Pretraining layer 1: 15 -> 64\n",
            "AE epoch 1/30 loss 1.001679\n",
            "AE epoch 10/30 loss 0.422423\n",
            "AE epoch 20/30 loss 0.139974\n",
            "AE epoch 30/30 loss 0.058336\n",
            "\n",
            "Pretraining layer 2: 64 -> 32\n",
            "AE epoch 1/30 loss 0.489889\n",
            "AE epoch 10/30 loss 0.197786\n",
            "AE epoch 20/30 loss 0.103182\n",
            "AE epoch 30/30 loss 0.060854\n",
            "\n",
            "Pretraining layer 3: 32 -> 16\n",
            "AE epoch 1/30 loss 1.100480\n",
            "AE epoch 10/30 loss 0.376653\n",
            "AE epoch 20/30 loss 0.248061\n",
            "AE epoch 30/30 loss 0.202386\n",
            "\n",
            "Pretraining layer 4: 16 -> 8\n",
            "AE epoch 1/30 loss 1.749125\n",
            "AE epoch 10/30 loss 0.942819\n",
            "AE epoch 20/30 loss 0.321574\n",
            "AE epoch 30/30 loss 0.141986\n",
            "\n",
            "--- Initializing MLP from pretrained encoders and finetune ---\n",
            "Epoch 1/40 train_loss=0.7616 val_loss=0.7616 val_f1=0.6125\n",
            "Epoch 5/40 train_loss=0.7234 val_loss=0.7246 val_f1=0.6125\n",
            "Epoch 10/40 train_loss=0.6852 val_loss=0.6871 val_f1=0.6125\n",
            "Epoch 15/40 train_loss=0.6562 val_loss=0.6586 val_f1=0.6906\n",
            "Epoch 20/40 train_loss=0.6311 val_loss=0.6337 val_f1=0.7778\n",
            "Epoch 25/40 train_loss=0.6073 val_loss=0.6083 val_f1=0.7835\n",
            "Epoch 30/40 train_loss=0.5822 val_loss=0.5816 val_f1=0.7912\n",
            "Epoch 35/40 train_loss=0.5554 val_loss=0.5540 val_f1=0.8043\n",
            "Epoch 40/40 train_loss=0.5302 val_loss=0.5262 val_f1=0.8000\n",
            "Pretrained eval: {'accuracy': 0.7898550724637681, 'precision': 0.8809523809523809, 'recall': 0.6065573770491803, 'f1': 0.7184466019417476, 'roc_auc': np.float64(0.9046199701937407), 'confusion_matrix': array([[72,  5],\n",
            "       [24, 37]])}\n",
            "\n",
            "=== Summary for WDBC ===\n",
            "Scratch accuracy : 0.956140350877193\n",
            "Scratch precision : 0.9855072463768116\n",
            "Scratch recall : 0.9444444444444444\n",
            "Scratch f1 : 0.9645390070921985\n",
            "Scratch roc_auc : 0.9943783068783069\n",
            "Scratch confusion_matrix : [[41  1]\n",
            " [ 4 68]]\n",
            "Pretrained accuracy : 0.8596491228070176\n",
            "Pretrained precision : 1.0\n",
            "Pretrained recall : 0.7777777777777778\n",
            "Pretrained f1 : 0.875\n",
            "Pretrained roc_auc : 0.9867724867724867\n",
            "Pretrained confusion_matrix : [[42  0]\n",
            " [16 56]]\n",
            "\n",
            "=== Summary for CreditApproval ===\n",
            "Scratch accuracy : 0.8623188405797102\n",
            "Scratch precision : 0.85\n",
            "Scratch recall : 0.8360655737704918\n",
            "Scratch f1 : 0.8429752066115702\n",
            "Scratch roc_auc : 0.9416648924845646\n",
            "Scratch confusion_matrix : [[68  9]\n",
            " [10 51]]\n",
            "Pretrained accuracy : 0.7898550724637681\n",
            "Pretrained precision : 0.8809523809523809\n",
            "Pretrained recall : 0.6065573770491803\n",
            "Pretrained f1 : 0.7184466019417476\n",
            "Pretrained roc_auc : 0.9046199701937407\n",
            "Pretrained confusion_matrix : [[72  5]\n",
            " [24 37]]\n",
            "\n",
            "Saved summary to pretrain_compare_summary.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "def load_wdbc():\n",
        "    try:\n",
        "        from sklearn.datasets import load_breast_cancer\n",
        "        data = load_breast_cancer()\n",
        "        X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "        y = pd.Series(data.target)  # 0/1\n",
        "        return X, y\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(\"Не удалось загрузить WDBC через sklearn: \" + str(e))\n",
        "\n",
        "def load_credit_approval():\n",
        "\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data\"\n",
        "    df = pd.read_csv(url, header=None, na_values='?')\n",
        "    cols = [f\"c{i}\" for i in range(df.shape[1])]\n",
        "    df.columns = cols\n",
        "\n",
        "    X = df.iloc[:, :-1].copy()\n",
        "    y = df.iloc[:, -1].copy()\n",
        "\n",
        "    y = y.map({'+':1, '-':0})\n",
        "    return X, y\n",
        "\n",
        "def preprocess_mixed(X_raw, y_raw):\n",
        "\n",
        "    X = X_raw.copy()\n",
        "    y = y_raw.copy()\n",
        "\n",
        "    num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    cat_cols = [c for c in X.columns if c not in num_cols]\n",
        "\n",
        "    for c in num_cols:\n",
        "        X[c] = X[c].fillna(X[c].median())\n",
        "\n",
        "    for c in cat_cols:\n",
        "        X[c] = X[c].astype(str).fillna(\"NA\")\n",
        "        le = LabelEncoder()\n",
        "        X[c] = le.fit_transform(X[c])\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    if len(num_cols)>0:\n",
        "        X[num_cols] = scaler.fit_transform(X[num_cols])\n",
        "\n",
        "    if len(cat_cols)>0:\n",
        "        X[cat_cols] = scaler.fit_transform(X[cat_cols])\n",
        "\n",
        "    mask = ~y.isna()\n",
        "    X = X.loc[mask].reset_index(drop=True)\n",
        "    y = y.loc[mask].reset_index(drop=True).astype(int)\n",
        "    return X.values.astype(np.float32), y.values.astype(np.int64)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_sizes=[64,32,16,8], n_classes=2):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        layer_sizes = [input_dim] + hidden_sizes\n",
        "        for i in range(len(layer_sizes)-1):\n",
        "            layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
        "            layers.append(nn.ReLU())\n",
        "        self.features = nn.Sequential(*layers)\n",
        "        self.classifier = nn.Linear(layer_sizes[-1], n_classes)\n",
        "    def forward(self, x):\n",
        "        h = self.features(x)\n",
        "        out = self.classifier(h)\n",
        "        return out\n",
        "\n",
        "class AutoencoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Autoencoder for one layer: enc: in_dim -> hidden, dec: hidden -> in_dim\n",
        "    We'll use MSE loss to reconstruct.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(nn.Linear(in_dim, hidden_dim), nn.ReLU())\n",
        "        self.decoder = nn.Sequential(nn.Linear(hidden_dim, in_dim))\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        recon = self.decoder(z)\n",
        "        return recon\n",
        "    def encode(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "\n",
        "def train_classification(model, train_loader, val_loader, epochs=50, lr=1e-3, weight_decay=1e-5, print_every=5):\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    best_val_f1 = 0.0\n",
        "    best_state = None\n",
        "    history = {\"train_loss\":[], \"val_loss\":[], \"val_f1\":[]}\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        for xb, yb in train_loader:\n",
        "            xb = xb.to(device); yb = yb.to(device)\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "        ys_true = []; ys_pred = []\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb = xb.to(device); yb = yb.to(device)\n",
        "                logits = model(xb)\n",
        "                loss = criterion(logits, yb)\n",
        "                val_losses.append(loss.item())\n",
        "                preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "                ys_true.append(yb.cpu().numpy()); ys_pred.append(preds)\n",
        "        ys_true = np.concatenate(ys_true)\n",
        "        ys_pred = np.concatenate(ys_pred)\n",
        "        val_f1 = f1_score(ys_true, ys_pred, zero_division=0)\n",
        "        history[\"train_loss\"].append(np.mean(train_losses))\n",
        "        history[\"val_loss\"].append(np.mean(val_losses))\n",
        "        history[\"val_f1\"].append(val_f1)\n",
        "        if val_f1 > best_val_f1:\n",
        "            best_val_f1 = val_f1\n",
        "            best_state = model.state_dict()\n",
        "        if epoch % print_every == 0 or epoch==1 or epoch==epochs:\n",
        "            print(f\"Epoch {epoch}/{epochs} train_loss={history['train_loss'][-1]:.4f} val_loss={history['val_loss'][-1]:.4f} val_f1={val_f1:.4f}\")\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "    return model, history\n",
        "\n",
        "def train_autoencoder(ae_model, data_loader, epochs=50, lr=1e-3, weight_decay=1e-5, print_every=10):\n",
        "    ae_model = ae_model.to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(ae_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    for epoch in range(1, epochs+1):\n",
        "        ae_model.train()\n",
        "        losses = []\n",
        "        for xb in data_loader:\n",
        "            if isinstance(xb, (list,tuple)):\n",
        "                xb = xb[0]\n",
        "            xb = xb.to(device)\n",
        "            recon = ae_model(xb)\n",
        "            loss = criterion(recon, xb)\n",
        "            optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
        "            losses.append(loss.item())\n",
        "        if epoch % print_every == 0 or epoch==1 or epoch==epochs:\n",
        "            print(f\"AE epoch {epoch}/{epochs} loss {np.mean(losses):.6f}\")\n",
        "    return ae_model\n",
        "\n",
        "def layerwise_pretrain(X_train, hidden_sizes, ae_epochs=50, batch_size=64, lr=1e-3):\n",
        "    \"\"\"\n",
        "    X_train: numpy array, shape (N, D)\n",
        "    hidden_sizes: list of ints, e.g. [64,32,16,8]\n",
        "    Returns: list of encoder weight/state dicts to initialize MLP\n",
        "    \"\"\"\n",
        "    encoders = []\n",
        "    current_input = torch.tensor(X_train, dtype=torch.float32)\n",
        "    dataset = TensorDataset(current_input)\n",
        "    for i, hdim in enumerate(hidden_sizes):\n",
        "        in_dim = current_input.shape[1]\n",
        "        print(f\"\\nPretraining layer {i+1}: {in_dim} -> {hdim}\")\n",
        "        ae = AutoencoderLayer(in_dim, hdim)\n",
        "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "        ae = train_autoencoder(ae, loader, epochs=ae_epochs, lr=lr)\n",
        "\n",
        "        encoders.append(ae.encoder.state_dict())\n",
        "\n",
        "        ae = ae.to(device)\n",
        "        ae.eval()\n",
        "        with torch.no_grad():\n",
        "            encoded = []\n",
        "            for (xb,) in loader:\n",
        "                xb = xb.to(device)\n",
        "                z = ae.encode(xb).cpu()\n",
        "                encoded.append(z)\n",
        "            encoded = torch.cat(encoded, dim=0)\n",
        "\n",
        "        current_input = encoded\n",
        "        dataset = TensorDataset(current_input)\n",
        "    return encoders\n",
        "\n",
        "\n",
        "def init_mlp_from_encoders(mlp_model, encoders_states):\n",
        "    \"\"\"\n",
        "    mlp_model.features includes layers like Linear, ReLU, Linear, ReLU...\n",
        "    encoders_states: list of state_dicts for each encoder (Linear + ReLU) saved from AutoencoderLayer.encoder\n",
        "    We'll assign weight & bias to corresponding Linear layers.\n",
        "    \"\"\"\n",
        "    feat = mlp_model.features\n",
        "    linear_layers = [m for m in feat.modules() if isinstance(m, nn.Linear)]\n",
        "\n",
        "    for i, state in enumerate(encoders_states):\n",
        "        if i < len(linear_layers):\n",
        "            linear_layers[i].weight.data = state['0.weight'].data.clone()\n",
        "            linear_layers[i].bias.data = state['0.bias'].data.clone()\n",
        "        else:\n",
        "            print(\"Warning: more encoders than linear layers in MLP\")\n",
        "    return mlp_model\n",
        "\n",
        "\n",
        "def evaluate_model(model, loader):\n",
        "    model.eval()\n",
        "    ys_true=[]; ys_pred=[]; ys_prob=[]\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(device)\n",
        "            logits = model(xb)\n",
        "            probs = torch.softmax(logits, dim=1)[:,1].cpu().numpy()\n",
        "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "            ys_prob.append(probs); ys_pred.append(preds); ys_true.append(yb.numpy())\n",
        "    y_true = np.concatenate(ys_true)\n",
        "    y_pred = np.concatenate(ys_pred)\n",
        "    y_prob = np.concatenate(ys_prob)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
        "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "    try:\n",
        "        roc = roc_auc_score(y_true, y_prob)\n",
        "    except:\n",
        "        roc = np.nan\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    return {\"accuracy\":acc, \"precision\":prec, \"recall\":rec, \"f1\":f1, \"roc_auc\":roc, \"confusion_matrix\":cm}\n",
        "\n",
        "\n",
        "def run_experiment(X, y, dataset_name=\"dataset\", hidden_sizes=[64,32,16,8], epochs_sup=50, ae_epochs=50, test_size=0.2, val_size=0.2, batch_size=64):\n",
        "    print(f\"\\n==== Dataset: {dataset_name} | N={X.shape[0]} F={X.shape[1]} ====\")\n",
        "\n",
        "    X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=test_size, stratify=y, random_state=42)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=val_size, stratify=y_trainval, random_state=42)\n",
        "    print(\"Split sizes:\", X_train.shape[0], X_val.shape[0], X_test.shape[0])\n",
        "\n",
        "    def make_loader(Xa, ya, batch_size=batch_size, shuffle=True):\n",
        "        ds = TensorDataset(torch.tensor(Xa, dtype=torch.float32), torch.tensor(ya, dtype=torch.long))\n",
        "        return DataLoader(ds, batch_size=batch_size, shuffle=shuffle)\n",
        "    train_loader = make_loader(X_train, y_train)\n",
        "    val_loader = make_loader(X_val, y_val, shuffle=False)\n",
        "    test_loader = make_loader(X_test, y_test, shuffle=False)\n",
        "\n",
        "    input_dim = X.shape[1]\n",
        "    n_classes = len(np.unique(y))\n",
        "\n",
        "    print(\"\\n--- Training from scratch ---\")\n",
        "    model_scratch = MLP(input_dim, hidden_sizes=hidden_sizes, n_classes=n_classes)\n",
        "    model_scratch, hist_scratch = train_classification(model_scratch, train_loader, val_loader, epochs=epochs_sup, lr=1e-3)\n",
        "    eval_scratch = evaluate_model(model_scratch, test_loader)\n",
        "    print(\"Scratch eval:\", eval_scratch)\n",
        "\n",
        "    print(\"\\n--- Layer-wise pretraining autoencoders ---\")\n",
        "    encoders_states = layerwise_pretrain(X_train, hidden_sizes, ae_epochs=ae_epochs, batch_size=batch_size, lr=1e-3)\n",
        "\n",
        "\n",
        "    print(\"\\n--- Initializing MLP from pretrained encoders and finetune ---\")\n",
        "    model_pre = MLP(input_dim, hidden_sizes=hidden_sizes, n_classes=n_classes)\n",
        "    model_pre = init_mlp_from_encoders(model_pre, encoders_states)\n",
        "\n",
        "    model_pre, hist_pre = train_classification(model_pre, train_loader, val_loader, epochs=epochs_sup, lr=1e-4)\n",
        "    eval_pre = evaluate_model(model_pre, test_loader)\n",
        "    print(\"Pretrained eval:\", eval_pre)\n",
        "\n",
        "\n",
        "    results = {\n",
        "        \"scratch\": eval_scratch,\n",
        "        \"pretrained\": eval_pre,\n",
        "        \"hist_scratch\": hist_scratch,\n",
        "        \"hist_pre\": hist_pre,\n",
        "        \"model_scratch\": model_scratch,\n",
        "        \"model_pre\": model_pre\n",
        "    }\n",
        "    return results\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    hidden_sizes = [64, 32, 16, 8]\n",
        "    epochs_sup = 40\n",
        "    ae_epochs = 30\n",
        "    batch_size = 64\n",
        "\n",
        "\n",
        "    X_wdbc, y_wdbc = load_wdbc()\n",
        "    Xw, yw = preprocess_mixed(X_wdbc, y_wdbc)\n",
        "    res_wdbc = run_experiment(Xw, yw, dataset_name=\"WDBC\", hidden_sizes=hidden_sizes, epochs_sup=epochs_sup, ae_epochs=ae_epochs, batch_size=batch_size)\n",
        "\n",
        "\n",
        "    X_cr, y_cr = load_credit_approval()\n",
        "    Xc, yc = preprocess_mixed(X_cr, y_cr)\n",
        "    res_credit = run_experiment(Xc, yc, dataset_name=\"CreditApproval\", hidden_sizes=hidden_sizes, epochs_sup=epochs_sup, ae_epochs=ae_epochs, batch_size=batch_size)\n",
        "\n",
        "    def print_summary(name, res):\n",
        "        print(f\"\\n=== Summary for {name} ===\")\n",
        "        for key in [\"accuracy\",\"precision\",\"recall\",\"f1\",\"roc_auc\",\"confusion_matrix\"]:\n",
        "            print(\"Scratch\", key, \":\", res[\"scratch\"].get(key))\n",
        "        for key in [\"accuracy\",\"precision\",\"recall\",\"f1\",\"roc_auc\",\"confusion_matrix\"]:\n",
        "            print(\"Pretrained\", key, \":\", res[\"pretrained\"].get(key))\n",
        "    print_summary(\"WDBC\", res_wdbc)\n",
        "    print_summary(\"CreditApproval\", res_credit)\n",
        "\n",
        "    summary = []\n",
        "    for name, r in [(\"WDBC\",res_wdbc), (\"CreditApproval\",res_credit)]:\n",
        "        for mode in [\"scratch\",\"pretrained\"]:\n",
        "            d = r[mode]\n",
        "            summary.append({\n",
        "                \"dataset\": name,\n",
        "                \"mode\": mode,\n",
        "                \"accuracy\": d[\"accuracy\"],\n",
        "                \"precision\": d[\"precision\"],\n",
        "                \"recall\": d[\"recall\"],\n",
        "                \"f1\": d[\"f1\"],\n",
        "                \"roc_auc\": d[\"roc_auc\"],\n",
        "                \"cm\": d[\"confusion_matrix\"].tolist()\n",
        "            })\n",
        "    df_summary = pd.DataFrame(summary)\n",
        "    df_summary.to_csv(\"pretrain_compare_summary.csv\", index=False)\n",
        "    print(\"\\nSaved summary to pretrain_compare_summary.csv\")\n"
      ]
    }
  ]
}