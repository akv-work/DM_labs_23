{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDkLpvKRXENe",
        "outputId": "d2e52db8-edf3-46c4-f4d6-a30ff5eb927c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n",
            "\n",
            "==== Dataset: WDBC | N=569 F=30 ====\n",
            "Split sizes: 364 91 114\n",
            "\n",
            "--- Training from scratch ---\n",
            "Epoch 1/40 train_loss=0.7143 val_loss=0.7042 val_f1=0.0000\n",
            "Epoch 5/40 train_loss=0.5943 val_loss=0.5768 val_f1=0.9483\n",
            "Epoch 10/40 train_loss=0.1919 val_loss=0.1661 val_f1=0.9739\n",
            "Epoch 15/40 train_loss=0.0674 val_loss=0.0813 val_f1=0.9825\n",
            "Epoch 20/40 train_loss=0.0444 val_loss=0.0929 val_f1=0.9825\n",
            "Epoch 25/40 train_loss=0.0312 val_loss=0.0932 val_f1=0.9735\n",
            "Epoch 30/40 train_loss=0.0239 val_loss=0.0948 val_f1=0.9735\n",
            "Epoch 35/40 train_loss=0.0187 val_loss=0.0880 val_f1=0.9825\n",
            "Epoch 40/40 train_loss=0.0147 val_loss=0.0935 val_f1=0.9735\n",
            "Scratch eval: {'accuracy': 0.956140350877193, 'precision': 0.9855072463768116, 'recall': 0.9444444444444444, 'f1': 0.9645390070921985, 'roc_auc': np.float64(0.9943783068783069), 'confusion_matrix': array([[41,  1],\n",
            "       [ 4, 68]])}\n",
            "\n",
            "--- Layer-wise pretraining: Autoencoders ---\n",
            "\n",
            "Pretraining AE layer 1: 30 -> 64\n",
            "AE epoch 1/30 loss 1.099238\n",
            "AE epoch 10/30 loss 0.323581\n",
            "AE epoch 20/30 loss 0.144776\n",
            "AE epoch 30/30 loss 0.088569\n",
            "\n",
            "Pretraining AE layer 2: 64 -> 32\n",
            "AE epoch 1/30 loss 0.640508\n",
            "AE epoch 10/30 loss 0.195796\n",
            "AE epoch 20/30 loss 0.094007\n",
            "AE epoch 30/30 loss 0.061484\n",
            "\n",
            "Pretraining AE layer 3: 32 -> 16\n",
            "AE epoch 1/30 loss 1.224143\n",
            "AE epoch 10/30 loss 0.388969\n",
            "AE epoch 20/30 loss 0.222152\n",
            "AE epoch 30/30 loss 0.141597\n",
            "\n",
            "Pretraining AE layer 4: 16 -> 8\n",
            "AE epoch 1/30 loss 2.004041\n",
            "AE epoch 10/30 loss 1.760936\n",
            "AE epoch 20/30 loss 0.989153\n",
            "AE epoch 30/30 loss 0.447652\n",
            "\n",
            "--- Initializing MLP from pretrained autoencoders and finetune ---\n",
            "Epoch 1/40 train_loss=0.7672 val_loss=0.7600 val_f1=0.0000\n",
            "Epoch 5/40 train_loss=0.7373 val_loss=0.7299 val_f1=0.0000\n",
            "Epoch 10/40 train_loss=0.6997 val_loss=0.6986 val_f1=0.0000\n",
            "Epoch 15/40 train_loss=0.6711 val_loss=0.6691 val_f1=0.0000\n",
            "Epoch 20/40 train_loss=0.6383 val_loss=0.6383 val_f1=0.0345\n",
            "Epoch 25/40 train_loss=0.6058 val_loss=0.6054 val_f1=0.4384\n",
            "Epoch 30/40 train_loss=0.5705 val_loss=0.5690 val_f1=0.8000\n",
            "Epoch 35/40 train_loss=0.5259 val_loss=0.5272 val_f1=0.9259\n",
            "Epoch 40/40 train_loss=0.4829 val_loss=0.4799 val_f1=0.9464\n",
            "AE Pretrained eval: {'accuracy': 0.8596491228070176, 'precision': 1.0, 'recall': 0.7777777777777778, 'f1': 0.875, 'roc_auc': np.float64(0.9867724867724867), 'confusion_matrix': array([[42,  0],\n",
            "       [16, 56]])}\n",
            "\n",
            "--- Layer-wise pretraining: RBMs ---\n",
            "\n",
            "Pretraining RBM layer 1: 30 -> 64\n",
            "[BernoulliRBM] Iteration 1, pseudo-likelihood = -15.04, time = 0.01s\n",
            "[BernoulliRBM] Iteration 2, pseudo-likelihood = -13.90, time = 0.01s\n",
            "[BernoulliRBM] Iteration 3, pseudo-likelihood = -13.40, time = 0.01s\n",
            "[BernoulliRBM] Iteration 4, pseudo-likelihood = -13.14, time = 0.01s\n",
            "[BernoulliRBM] Iteration 5, pseudo-likelihood = -13.09, time = 0.01s\n",
            "[BernoulliRBM] Iteration 6, pseudo-likelihood = -13.01, time = 0.01s\n",
            "[BernoulliRBM] Iteration 7, pseudo-likelihood = -13.02, time = 0.01s\n",
            "[BernoulliRBM] Iteration 8, pseudo-likelihood = -12.93, time = 0.01s\n",
            "[BernoulliRBM] Iteration 9, pseudo-likelihood = -12.89, time = 0.01s\n",
            "[BernoulliRBM] Iteration 10, pseudo-likelihood = -12.87, time = 0.01s\n",
            "\n",
            "Pretraining RBM layer 2: 64 -> 32\n",
            "[BernoulliRBM] Iteration 1, pseudo-likelihood = -43.37, time = 0.00s\n",
            "[BernoulliRBM] Iteration 2, pseudo-likelihood = -42.98, time = 0.01s\n",
            "[BernoulliRBM] Iteration 3, pseudo-likelihood = -42.79, time = 0.01s\n",
            "[BernoulliRBM] Iteration 4, pseudo-likelihood = -42.64, time = 0.01s\n",
            "[BernoulliRBM] Iteration 5, pseudo-likelihood = -42.56, time = 0.01s\n",
            "[BernoulliRBM] Iteration 6, pseudo-likelihood = -42.46, time = 0.01s\n",
            "[BernoulliRBM] Iteration 7, pseudo-likelihood = -42.42, time = 0.01s\n",
            "[BernoulliRBM] Iteration 8, pseudo-likelihood = -42.39, time = 0.01s\n",
            "[BernoulliRBM] Iteration 9, pseudo-likelihood = -42.38, time = 0.01s\n",
            "[BernoulliRBM] Iteration 10, pseudo-likelihood = -42.37, time = 0.01s\n",
            "\n",
            "Pretraining RBM layer 3: 32 -> 16\n",
            "[BernoulliRBM] Iteration 1, pseudo-likelihood = -20.39, time = 0.00s\n",
            "[BernoulliRBM] Iteration 2, pseudo-likelihood = -19.49, time = 0.00s\n",
            "[BernoulliRBM] Iteration 3, pseudo-likelihood = -18.91, time = 0.00s\n",
            "[BernoulliRBM] Iteration 4, pseudo-likelihood = -18.50, time = 0.01s\n",
            "[BernoulliRBM] Iteration 5, pseudo-likelihood = -18.17, time = 0.01s\n",
            "[BernoulliRBM] Iteration 6, pseudo-likelihood = -17.92, time = 0.00s\n",
            "[BernoulliRBM] Iteration 7, pseudo-likelihood = -17.69, time = 0.00s\n",
            "[BernoulliRBM] Iteration 8, pseudo-likelihood = -17.54, time = 0.00s\n",
            "[BernoulliRBM] Iteration 9, pseudo-likelihood = -17.41, time = 0.00s\n",
            "[BernoulliRBM] Iteration 10, pseudo-likelihood = -17.30, time = 0.00s\n",
            "\n",
            "Pretraining RBM layer 4: 16 -> 8\n",
            "[BernoulliRBM] Iteration 1, pseudo-likelihood = -10.62, time = 0.00s\n",
            "[BernoulliRBM] Iteration 2, pseudo-likelihood = -10.33, time = 0.00s\n",
            "[BernoulliRBM] Iteration 3, pseudo-likelihood = -10.13, time = 0.00s\n",
            "[BernoulliRBM] Iteration 4, pseudo-likelihood = -9.96, time = 0.00s\n",
            "[BernoulliRBM] Iteration 5, pseudo-likelihood = -9.83, time = 0.00s\n",
            "[BernoulliRBM] Iteration 6, pseudo-likelihood = -9.72, time = 0.00s\n",
            "[BernoulliRBM] Iteration 7, pseudo-likelihood = -9.63, time = 0.00s\n",
            "[BernoulliRBM] Iteration 8, pseudo-likelihood = -9.57, time = 0.00s\n",
            "[BernoulliRBM] Iteration 9, pseudo-likelihood = -9.53, time = 0.00s\n",
            "[BernoulliRBM] Iteration 10, pseudo-likelihood = -9.47, time = 0.00s\n",
            "\n",
            "--- Initializing MLP from pretrained RBMs and finetune ---\n",
            "Epoch 1/40 train_loss=0.6756 val_loss=0.6761 val_f1=0.7703\n",
            "Epoch 5/40 train_loss=0.6754 val_loss=0.6757 val_f1=0.7703\n",
            "Epoch 10/40 train_loss=0.6756 val_loss=0.6753 val_f1=0.7703\n",
            "Epoch 15/40 train_loss=0.6748 val_loss=0.6748 val_f1=0.7703\n",
            "Epoch 20/40 train_loss=0.6745 val_loss=0.6744 val_f1=0.7703\n",
            "Epoch 25/40 train_loss=0.6755 val_loss=0.6740 val_f1=0.7703\n",
            "Epoch 30/40 train_loss=0.6740 val_loss=0.6737 val_f1=0.7703\n",
            "Epoch 35/40 train_loss=0.6734 val_loss=0.6733 val_f1=0.7703\n",
            "Epoch 40/40 train_loss=0.6730 val_loss=0.6729 val_f1=0.7703\n",
            "RBM Pretrained eval: {'accuracy': 0.631578947368421, 'precision': 0.631578947368421, 'recall': 1.0, 'f1': 0.7741935483870968, 'roc_auc': np.float64(0.5), 'confusion_matrix': array([[ 0, 42],\n",
            "       [ 0, 72]])}\n",
            "\n",
            "==== Dataset: CreditApproval | N=690 F=15 ====\n",
            "Split sizes: 441 111 138\n",
            "\n",
            "--- Training from scratch ---\n",
            "Epoch 1/40 train_loss=0.7086 val_loss=0.7059 val_f1=0.6125\n",
            "Epoch 5/40 train_loss=0.6811 val_loss=0.6720 val_f1=0.6316\n",
            "Epoch 10/40 train_loss=0.4935 val_loss=0.4494 val_f1=0.8667\n",
            "Epoch 15/40 train_loss=0.3462 val_loss=0.3193 val_f1=0.8750\n",
            "Epoch 20/40 train_loss=0.3126 val_loss=0.2922 val_f1=0.8842\n",
            "Epoch 25/40 train_loss=0.2936 val_loss=0.2846 val_f1=0.8723\n",
            "Epoch 30/40 train_loss=0.2748 val_loss=0.2815 val_f1=0.8723\n",
            "Epoch 35/40 train_loss=0.2571 val_loss=0.2783 val_f1=0.8817\n",
            "Epoch 40/40 train_loss=0.2353 val_loss=0.2770 val_f1=0.8817\n",
            "Scratch eval: {'accuracy': 0.8913043478260869, 'precision': 0.859375, 'recall': 0.9016393442622951, 'f1': 0.88, 'roc_auc': np.float64(0.9520970832446242), 'confusion_matrix': array([[68,  9],\n",
            "       [ 6, 55]])}\n",
            "\n",
            "--- Layer-wise pretraining: Autoencoders ---\n",
            "\n",
            "Pretraining AE layer 1: 15 -> 64\n",
            "AE epoch 1/30 loss 1.017450\n",
            "AE epoch 10/30 loss 0.451695\n",
            "AE epoch 20/30 loss 0.163276\n",
            "AE epoch 30/30 loss 0.067016\n",
            "\n",
            "Pretraining AE layer 2: 64 -> 32\n",
            "AE epoch 1/30 loss 0.439885\n",
            "AE epoch 10/30 loss 0.200020\n",
            "AE epoch 20/30 loss 0.101035\n",
            "AE epoch 30/30 loss 0.056365\n",
            "\n",
            "Pretraining AE layer 3: 32 -> 16\n",
            "AE epoch 1/30 loss 1.207882\n",
            "AE epoch 10/30 loss 0.407012\n",
            "AE epoch 20/30 loss 0.272588\n",
            "AE epoch 30/30 loss 0.237393\n",
            "\n",
            "Pretraining AE layer 4: 16 -> 8\n",
            "AE epoch 1/30 loss 1.395242\n",
            "AE epoch 10/30 loss 0.518655\n",
            "AE epoch 20/30 loss 0.150982\n",
            "AE epoch 30/30 loss 0.083678\n",
            "\n",
            "--- Initializing MLP from pretrained autoencoders and finetune ---\n",
            "Epoch 1/40 train_loss=0.9903 val_loss=0.9953 val_f1=0.6125\n",
            "Epoch 5/40 train_loss=0.9125 val_loss=0.9153 val_f1=0.6125\n",
            "Epoch 10/40 train_loss=0.8413 val_loss=0.8436 val_f1=0.6125\n",
            "Epoch 15/40 train_loss=0.7899 val_loss=0.7902 val_f1=0.6125\n",
            "Epoch 20/40 train_loss=0.7522 val_loss=0.7492 val_f1=0.6125\n",
            "Epoch 25/40 train_loss=0.7200 val_loss=0.7156 val_f1=0.6125\n",
            "Epoch 30/40 train_loss=0.6933 val_loss=0.6858 val_f1=0.6164\n",
            "Epoch 35/40 train_loss=0.6672 val_loss=0.6570 val_f1=0.6400\n",
            "Epoch 40/40 train_loss=0.6390 val_loss=0.6273 val_f1=0.6870\n",
            "AE Pretrained eval: {'accuracy': 0.6304347826086957, 'precision': 0.55, 'recall': 0.9016393442622951, 'f1': 0.6832298136645962, 'roc_auc': np.float64(0.8237172663402172), 'confusion_matrix': array([[32, 45],\n",
            "       [ 6, 55]])}\n",
            "\n",
            "--- Layer-wise pretraining: RBMs ---\n",
            "\n",
            "Pretraining RBM layer 1: 15 -> 64\n",
            "[BernoulliRBM] Iteration 1, pseudo-likelihood = -7.75, time = 0.00s\n",
            "[BernoulliRBM] Iteration 2, pseudo-likelihood = -7.44, time = 0.01s\n",
            "[BernoulliRBM] Iteration 3, pseudo-likelihood = -7.60, time = 0.01s\n",
            "[BernoulliRBM] Iteration 4, pseudo-likelihood = -7.37, time = 0.01s\n",
            "[BernoulliRBM] Iteration 5, pseudo-likelihood = -7.17, time = 0.01s\n",
            "[BernoulliRBM] Iteration 6, pseudo-likelihood = -7.28, time = 0.01s\n",
            "[BernoulliRBM] Iteration 7, pseudo-likelihood = -7.35, time = 0.01s\n",
            "[BernoulliRBM] Iteration 8, pseudo-likelihood = -7.18, time = 0.01s\n",
            "[BernoulliRBM] Iteration 9, pseudo-likelihood = -7.27, time = 0.01s\n",
            "[BernoulliRBM] Iteration 10, pseudo-likelihood = -7.09, time = 0.01s\n",
            "\n",
            "Pretraining RBM layer 2: 64 -> 32\n",
            "[BernoulliRBM] Iteration 1, pseudo-likelihood = -41.05, time = 0.01s\n",
            "[BernoulliRBM] Iteration 2, pseudo-likelihood = -40.34, time = 0.01s\n",
            "[BernoulliRBM] Iteration 3, pseudo-likelihood = -40.02, time = 0.01s\n",
            "[BernoulliRBM] Iteration 4, pseudo-likelihood = -39.89, time = 0.01s\n",
            "[BernoulliRBM] Iteration 5, pseudo-likelihood = -39.72, time = 0.01s\n",
            "[BernoulliRBM] Iteration 6, pseudo-likelihood = -39.67, time = 0.01s\n",
            "[BernoulliRBM] Iteration 7, pseudo-likelihood = -39.59, time = 0.01s\n",
            "[BernoulliRBM] Iteration 8, pseudo-likelihood = -39.47, time = 0.01s\n",
            "[BernoulliRBM] Iteration 9, pseudo-likelihood = -39.44, time = 0.01s\n",
            "[BernoulliRBM] Iteration 10, pseudo-likelihood = -39.42, time = 0.01s\n",
            "\n",
            "Pretraining RBM layer 3: 32 -> 16\n",
            "[BernoulliRBM] Iteration 1, pseudo-likelihood = -14.79, time = 0.00s\n",
            "[BernoulliRBM] Iteration 2, pseudo-likelihood = -11.66, time = 0.01s\n",
            "[BernoulliRBM] Iteration 3, pseudo-likelihood = -9.77, time = 0.01s\n",
            "[BernoulliRBM] Iteration 4, pseudo-likelihood = -8.46, time = 0.01s\n",
            "[BernoulliRBM] Iteration 5, pseudo-likelihood = -7.54, time = 0.01s\n",
            "[BernoulliRBM] Iteration 6, pseudo-likelihood = -6.83, time = 0.01s\n",
            "[BernoulliRBM] Iteration 7, pseudo-likelihood = -6.43, time = 0.01s\n",
            "[BernoulliRBM] Iteration 8, pseudo-likelihood = -6.03, time = 0.01s\n",
            "[BernoulliRBM] Iteration 9, pseudo-likelihood = -5.83, time = 0.01s\n",
            "[BernoulliRBM] Iteration 10, pseudo-likelihood = -5.65, time = 0.01s\n",
            "\n",
            "Pretraining RBM layer 4: 16 -> 8\n",
            "[BernoulliRBM] Iteration 1, pseudo-likelihood = -10.76, time = 0.00s\n",
            "[BernoulliRBM] Iteration 2, pseudo-likelihood = -10.60, time = 0.00s\n",
            "[BernoulliRBM] Iteration 3, pseudo-likelihood = -10.49, time = 0.00s\n",
            "[BernoulliRBM] Iteration 4, pseudo-likelihood = -10.40, time = 0.00s\n",
            "[BernoulliRBM] Iteration 5, pseudo-likelihood = -10.34, time = 0.00s\n",
            "[BernoulliRBM] Iteration 6, pseudo-likelihood = -10.29, time = 0.00s\n",
            "[BernoulliRBM] Iteration 7, pseudo-likelihood = -10.27, time = 0.00s\n",
            "[BernoulliRBM] Iteration 8, pseudo-likelihood = -10.24, time = 0.00s\n",
            "[BernoulliRBM] Iteration 9, pseudo-likelihood = -10.21, time = 0.00s\n",
            "[BernoulliRBM] Iteration 10, pseudo-likelihood = -10.20, time = 0.00s\n",
            "\n",
            "--- Initializing MLP from pretrained RBMs and finetune ---\n",
            "Epoch 1/40 train_loss=0.7023 val_loss=0.7024 val_f1=0.6125\n",
            "Epoch 5/40 train_loss=0.7018 val_loss=0.7020 val_f1=0.6125\n",
            "Epoch 10/40 train_loss=0.7014 val_loss=0.7015 val_f1=0.6125\n",
            "Epoch 15/40 train_loss=0.7009 val_loss=0.7011 val_f1=0.6125\n",
            "Epoch 20/40 train_loss=0.7005 val_loss=0.7006 val_f1=0.6125\n",
            "Epoch 25/40 train_loss=0.7000 val_loss=0.7001 val_f1=0.6125\n",
            "Epoch 30/40 train_loss=0.6996 val_loss=0.6997 val_f1=0.6125\n",
            "Epoch 35/40 train_loss=0.6992 val_loss=0.6993 val_f1=0.6125\n",
            "Epoch 40/40 train_loss=0.6987 val_loss=0.6988 val_f1=0.6125\n",
            "RBM Pretrained eval: {'accuracy': 0.4420289855072464, 'precision': 0.4420289855072464, 'recall': 1.0, 'f1': 0.6130653266331658, 'roc_auc': np.float64(0.5), 'confusion_matrix': array([[ 0, 77],\n",
            "       [ 0, 61]])}\n",
            "\n",
            "=== Summary for WDBC ===\n",
            "\n",
            "--- scratch ---\n",
            "accuracy : 0.956140350877193\n",
            "precision : 0.9855072463768116\n",
            "recall : 0.9444444444444444\n",
            "f1 : 0.9645390070921985\n",
            "roc_auc : 0.9943783068783069\n",
            "confusion_matrix : [[41  1]\n",
            " [ 4 68]]\n",
            "\n",
            "--- pretrained_ae ---\n",
            "accuracy : 0.8596491228070176\n",
            "precision : 1.0\n",
            "recall : 0.7777777777777778\n",
            "f1 : 0.875\n",
            "roc_auc : 0.9867724867724867\n",
            "confusion_matrix : [[42  0]\n",
            " [16 56]]\n",
            "\n",
            "--- pretrained_rbm ---\n",
            "accuracy : 0.631578947368421\n",
            "precision : 0.631578947368421\n",
            "recall : 1.0\n",
            "f1 : 0.7741935483870968\n",
            "roc_auc : 0.5\n",
            "confusion_matrix : [[ 0 42]\n",
            " [ 0 72]]\n",
            "\n",
            "=== Summary for CreditApproval ===\n",
            "\n",
            "--- scratch ---\n",
            "accuracy : 0.8913043478260869\n",
            "precision : 0.859375\n",
            "recall : 0.9016393442622951\n",
            "f1 : 0.88\n",
            "roc_auc : 0.9520970832446242\n",
            "confusion_matrix : [[68  9]\n",
            " [ 6 55]]\n",
            "\n",
            "--- pretrained_ae ---\n",
            "accuracy : 0.6304347826086957\n",
            "precision : 0.55\n",
            "recall : 0.9016393442622951\n",
            "f1 : 0.6832298136645962\n",
            "roc_auc : 0.8237172663402172\n",
            "confusion_matrix : [[32 45]\n",
            " [ 6 55]]\n",
            "\n",
            "--- pretrained_rbm ---\n",
            "accuracy : 0.4420289855072464\n",
            "precision : 0.4420289855072464\n",
            "recall : 1.0\n",
            "f1 : 0.6130653266331658\n",
            "roc_auc : 0.5\n",
            "confusion_matrix : [[ 0 77]\n",
            " [ 0 61]]\n",
            "\n",
            "Saved summary to pretrain_compare_summary_rbm.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "from sklearn.neural_network import BernoulliRBM\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "def load_wdbc():\n",
        "    try:\n",
        "        from sklearn.datasets import load_breast_cancer\n",
        "        data = load_breast_cancer()\n",
        "        X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "        y = pd.Series(data.target)  # 0/1\n",
        "        return X, y\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(\"Не удалось загрузить WDBC через sklearn: \" + str(e))\n",
        "\n",
        "def load_credit_approval():\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data\"\n",
        "    df = pd.read_csv(url, header=None, na_values='?')\n",
        "    cols = [f\"c{i}\" for i in range(df.shape[1])]\n",
        "    df.columns = cols\n",
        "    X = df.iloc[:, :-1].copy()\n",
        "    y = df.iloc[:, -1].copy()\n",
        "    y = y.map({'+':1, '-':0})\n",
        "    return X, y\n",
        "\n",
        "def preprocess_mixed(X_raw, y_raw, standardize=True):\n",
        "    X = X_raw.copy()\n",
        "    y = y_raw.copy()\n",
        "    num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    cat_cols = [c for c in X.columns if c not in num_cols]\n",
        "    for c in num_cols:\n",
        "        X[c] = X[c].fillna(X[c].median())\n",
        "    for c in cat_cols:\n",
        "        X[c] = X[c].astype(str).fillna(\"NA\")\n",
        "        le = LabelEncoder()\n",
        "        X[c] = le.fit_transform(X[c])\n",
        "    if standardize:\n",
        "        scaler = StandardScaler()\n",
        "        if len(num_cols)>0:\n",
        "            X[num_cols] = scaler.fit_transform(X[num_cols])\n",
        "        if len(cat_cols)>0:\n",
        "            X[cat_cols] = scaler.fit_transform(X[cat_cols])\n",
        "    mask = ~y.isna()\n",
        "    X = X.loc[mask].reset_index(drop=True)\n",
        "    y = y.loc[mask].reset_index(drop=True).astype(int)\n",
        "    return X.values.astype(np.float32), y.values.astype(np.int64)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_sizes=[64,32,16,8], n_classes=2):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        layer_sizes = [input_dim] + hidden_sizes\n",
        "        for i in range(len(layer_sizes)-1):\n",
        "            layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
        "            layers.append(nn.ReLU())\n",
        "        self.features = nn.Sequential(*layers)\n",
        "        self.classifier = nn.Linear(layer_sizes[-1], n_classes)\n",
        "    def forward(self, x):\n",
        "        h = self.features(x)\n",
        "        out = self.classifier(h)\n",
        "        return out\n",
        "\n",
        "class AutoencoderLayer(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(nn.Linear(in_dim, hidden_dim), nn.ReLU())\n",
        "        self.decoder = nn.Sequential(nn.Linear(hidden_dim, in_dim))\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        recon = self.decoder(z)\n",
        "        return recon\n",
        "    def encode(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "\n",
        "def train_classification(model, train_loader, val_loader, epochs=50, lr=1e-3, weight_decay=1e-5, print_every=5):\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    best_val_f1 = 0.0\n",
        "    best_state = None\n",
        "    history = {\"train_loss\":[], \"val_loss\":[], \"val_f1\":[]}\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        for xb, yb in train_loader:\n",
        "            xb = xb.to(device); yb = yb.to(device)\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "        # val\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "        ys_true = []; ys_pred = []\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb = xb.to(device); yb = yb.to(device)\n",
        "                logits = model(xb)\n",
        "                loss = criterion(logits, yb)\n",
        "                val_losses.append(loss.item())\n",
        "                preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "                ys_true.append(yb.cpu().numpy()); ys_pred.append(preds)\n",
        "        ys_true = np.concatenate(ys_true)\n",
        "        ys_pred = np.concatenate(ys_pred)\n",
        "        val_f1 = f1_score(ys_true, ys_pred, zero_division=0)\n",
        "        history[\"train_loss\"].append(np.mean(train_losses))\n",
        "        history[\"val_loss\"].append(np.mean(val_losses))\n",
        "        history[\"val_f1\"].append(val_f1)\n",
        "        if val_f1 > best_val_f1:\n",
        "            best_val_f1 = val_f1\n",
        "            best_state = model.state_dict()\n",
        "        if epoch % print_every == 0 or epoch==1 or epoch==epochs:\n",
        "            print(f\"Epoch {epoch}/{epochs} train_loss={history['train_loss'][-1]:.4f} val_loss={history['val_loss'][-1]:.4f} val_f1={val_f1:.4f}\")\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "    return model, history\n",
        "\n",
        "def train_autoencoder(ae_model, data_loader, epochs=50, lr=1e-3, weight_decay=1e-5, print_every=10):\n",
        "    ae_model = ae_model.to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(ae_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    for epoch in range(1, epochs+1):\n",
        "        ae_model.train()\n",
        "        losses = []\n",
        "        for xb in data_loader:\n",
        "            if isinstance(xb, (list,tuple)):\n",
        "                xb = xb[0]\n",
        "            xb = xb.to(device)\n",
        "            recon = ae_model(xb)\n",
        "            loss = criterion(recon, xb)\n",
        "            optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
        "            losses.append(loss.item())\n",
        "        if epoch % print_every == 0 or epoch==1 or epoch==epochs:\n",
        "            print(f\"AE epoch {epoch}/{epochs} loss {np.mean(losses):.6f}\")\n",
        "    return ae_model\n",
        "\n",
        "def layerwise_pretrain(X_train, hidden_sizes, ae_epochs=50, batch_size=64, lr=1e-3):\n",
        "    encoders = []\n",
        "    current_input = torch.tensor(X_train, dtype=torch.float32)\n",
        "    dataset = TensorDataset(current_input)\n",
        "    for i, hdim in enumerate(hidden_sizes):\n",
        "        in_dim = current_input.shape[1]\n",
        "        print(f\"\\nPretraining AE layer {i+1}: {in_dim} -> {hdim}\")\n",
        "        ae = AutoencoderLayer(in_dim, hdim)\n",
        "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "        ae = train_autoencoder(ae, loader, epochs=ae_epochs, lr=lr)\n",
        "        encoders.append(ae.encoder.state_dict())\n",
        "        ae = ae.to(device)\n",
        "        ae.eval()\n",
        "        with torch.no_grad():\n",
        "            encoded = []\n",
        "            for (xb,) in loader:\n",
        "                xb = xb.to(device)\n",
        "                z = ae.encode(xb).cpu()\n",
        "                encoded.append(z)\n",
        "            encoded = torch.cat(encoded, dim=0)\n",
        "        current_input = encoded\n",
        "        dataset = TensorDataset(current_input)\n",
        "    return encoders\n",
        "\n",
        "def rbm_layerwise_pretrain(X_train, hidden_sizes, rbm_epochs=10, batch_size=10, learning_rate=0.01, random_state=42, verbose=True):\n",
        "    \"\"\"\n",
        "    X_train: numpy array (N, D) with values in [0,1] (we'll scale outside)\n",
        "    Returns list of fitted RBM objects and list of hidden activations (for chaining)\n",
        "    \"\"\"\n",
        "    rbms = []\n",
        "    current_input = X_train.copy()\n",
        "    for i, hdim in enumerate(hidden_sizes):\n",
        "        in_dim = current_input.shape[1]\n",
        "        print(f\"\\nPretraining RBM layer {i+1}: {in_dim} -> {hdim}\")\n",
        "        rbm = BernoulliRBM(n_components=hdim, learning_rate=learning_rate, batch_size=batch_size, n_iter=rbm_epochs, verbose=verbose, random_state=random_state)\n",
        "        rbm.fit(current_input)\n",
        "        rbms.append(rbm)\n",
        "        hid = rbm.transform(current_input)\n",
        "        current_input = hid\n",
        "    return rbms\n",
        "\n",
        "def init_mlp_from_encoders(mlp_model, encoders_states):\n",
        "    feat = mlp_model.features\n",
        "    linear_layers = [m for m in feat.modules() if isinstance(m, nn.Linear)]\n",
        "    for i, state in enumerate(encoders_states):\n",
        "        if i < len(linear_layers):\n",
        "            linear_layers[i].weight.data = state['0.weight'].data.clone()\n",
        "            linear_layers[i].bias.data = state['0.bias'].data.clone()\n",
        "        else:\n",
        "            print(\"Warning: more encoders than linear layers in MLP\")\n",
        "    return mlp_model\n",
        "\n",
        "def init_mlp_from_rbms(mlp_model, rbms):\n",
        "    feat = mlp_model.features\n",
        "    linear_layers = [m for m in feat.modules() if isinstance(m, nn.Linear)]\n",
        "    for i, rbm in enumerate(rbms):\n",
        "        if i < len(linear_layers):\n",
        "            comp = rbm.components_.astype(np.float32)  # shape (hidden_dim, visible_dim)\n",
        "            intercept = rbm.intercept_hidden_.astype(np.float32)  # (hidden_dim,)\n",
        "            linear_layers[i].weight.data = torch.tensor(comp, dtype=torch.float32)\n",
        "            linear_layers[i].bias.data = torch.tensor(intercept, dtype=torch.float32)\n",
        "        else:\n",
        "            print(\"Warning: more RBMs than linear layers in MLP\")\n",
        "    return mlp_model\n",
        "\n",
        "def evaluate_model(model, loader):\n",
        "    model.eval()\n",
        "    ys_true=[]; ys_pred=[]; ys_prob=[]\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(device)\n",
        "            logits = model(xb)\n",
        "            probs = torch.softmax(logits, dim=1)[:,1].cpu().numpy()\n",
        "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "            ys_prob.append(probs); ys_pred.append(preds); ys_true.append(yb.numpy())\n",
        "    y_true = np.concatenate(ys_true)\n",
        "    y_pred = np.concatenate(ys_pred)\n",
        "    y_prob = np.concatenate(ys_prob)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
        "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "    try:\n",
        "        roc = roc_auc_score(y_true, y_prob)\n",
        "    except:\n",
        "        roc = np.nan\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    return {\"accuracy\":acc, \"precision\":prec, \"recall\":rec, \"f1\":f1, \"roc_auc\":roc, \"confusion_matrix\":cm}\n",
        "\n",
        "def run_experiment(X, y, dataset_name=\"dataset\", hidden_sizes=[64,32,16,8], epochs_sup=50, ae_epochs=50, rbm_epochs=10, test_size=0.2, val_size=0.2, batch_size=64):\n",
        "    print(f\"\\n==== Dataset: {dataset_name} | N={X.shape[0]} F={X.shape[1]} ====\")\n",
        "    X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=test_size, stratify=y, random_state=42)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=val_size, stratify=y_trainval, random_state=42)\n",
        "    print(\"Split sizes:\", X_train.shape[0], X_val.shape[0], X_test.shape[0])\n",
        "\n",
        "    def make_loader(Xa, ya, batch_size=batch_size, shuffle=True):\n",
        "        ds = TensorDataset(torch.tensor(Xa, dtype=torch.float32), torch.tensor(ya, dtype=torch.long))\n",
        "        return DataLoader(ds, batch_size=batch_size, shuffle=shuffle)\n",
        "    train_loader = make_loader(X_train, y_train)\n",
        "    val_loader = make_loader(X_val, y_val, shuffle=False)\n",
        "    test_loader = make_loader(X_test, y_test, shuffle=False)\n",
        "\n",
        "    input_dim = X.shape[1]\n",
        "    n_classes = len(np.unique(y))\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    print(\"\\n--- Training from scratch ---\")\n",
        "    model_scratch = MLP(input_dim, hidden_sizes=hidden_sizes, n_classes=n_classes)\n",
        "    model_scratch, hist_scratch = train_classification(model_scratch, train_loader, val_loader, epochs=epochs_sup, lr=1e-3)\n",
        "    eval_scratch = evaluate_model(model_scratch, test_loader)\n",
        "    print(\"Scratch eval:\", eval_scratch)\n",
        "    results['scratch'] = eval_scratch\n",
        "    results['hist_scratch'] = hist_scratch\n",
        "    results['model_scratch'] = model_scratch\n",
        "\n",
        "    print(\"\\n--- Layer-wise pretraining: Autoencoders ---\")\n",
        "    encoders_states = layerwise_pretrain(X_train, hidden_sizes, ae_epochs=ae_epochs, batch_size=batch_size, lr=1e-3)\n",
        "    print(\"\\n--- Initializing MLP from pretrained autoencoders and finetune ---\")\n",
        "    model_pre_ae = MLP(input_dim, hidden_sizes=hidden_sizes, n_classes=n_classes)\n",
        "    model_pre_ae = init_mlp_from_encoders(model_pre_ae, encoders_states)\n",
        "    model_pre_ae, hist_pre_ae = train_classification(model_pre_ae, train_loader, val_loader, epochs=epochs_sup, lr=1e-4)\n",
        "    eval_pre_ae = evaluate_model(model_pre_ae, test_loader)\n",
        "    print(\"AE Pretrained eval:\", eval_pre_ae)\n",
        "    results['pretrained_ae'] = eval_pre_ae\n",
        "    results['hist_pre_ae'] = hist_pre_ae\n",
        "    results['model_pre_ae'] = model_pre_ae\n",
        "\n",
        "    print(\"\\n--- Layer-wise pretraining: RBMs ---\")\n",
        "    mm = MinMaxScaler(feature_range=(0,1))\n",
        "    X_train_rbm = mm.fit_transform(X_train)\n",
        "    X_val_rbm = mm.transform(X_val)\n",
        "    X_test_rbm = mm.transform(X_test)\n",
        "\n",
        "    rbms = rbm_layerwise_pretrain(X_train_rbm, hidden_sizes, rbm_epochs=rbm_epochs, batch_size=min(10, X_train_rbm.shape[0]), learning_rate=0.01)\n",
        "    print(\"\\n--- Initializing MLP from pretrained RBMs and finetune ---\")\n",
        "    model_pre_rbm = MLP(input_dim, hidden_sizes=hidden_sizes, n_classes=n_classes)\n",
        "    model_pre_rbm = init_mlp_from_rbms(model_pre_rbm, rbms)\n",
        "    model_pre_rbm, hist_pre_rbm = train_classification(model_pre_rbm, train_loader, val_loader, epochs=epochs_sup, lr=1e-4)\n",
        "    eval_pre_rbm = evaluate_model(model_pre_rbm, test_loader)\n",
        "    print(\"RBM Pretrained eval:\", eval_pre_rbm)\n",
        "    results['pretrained_rbm'] = eval_pre_rbm\n",
        "    results['hist_pre_rbm'] = hist_pre_rbm\n",
        "    results['model_pre_rbm'] = model_pre_rbm\n",
        "\n",
        "    return results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    hidden_sizes = [64, 32, 16, 8]\n",
        "    epochs_sup = 40\n",
        "    ae_epochs = 30\n",
        "    rbm_epochs = 10\n",
        "    batch_size = 64\n",
        "\n",
        "    X_wdbc, y_wdbc = load_wdbc()\n",
        "    Xw, yw = preprocess_mixed(X_wdbc, y_wdbc, standardize=True)\n",
        "    res_wdbc = run_experiment(Xw, yw, dataset_name=\"WDBC\", hidden_sizes=hidden_sizes, epochs_sup=epochs_sup, ae_epochs=ae_epochs, rbm_epochs=rbm_epochs, batch_size=batch_size)\n",
        "\n",
        "    X_cr, y_cr = load_credit_approval()\n",
        "    Xc, yc = preprocess_mixed(X_cr, y_cr, standardize=True)\n",
        "    res_credit = run_experiment(Xc, yc, dataset_name=\"CreditApproval\", hidden_sizes=hidden_sizes, epochs_sup=epochs_sup, ae_epochs=ae_epochs, rbm_epochs=rbm_epochs, batch_size=batch_size)\n",
        "\n",
        "    def print_summary(name, res):\n",
        "        print(f\"\\n=== Summary for {name} ===\")\n",
        "        for mode in ['scratch', 'pretrained_ae', 'pretrained_rbm']:\n",
        "            print(f\"\\n--- {mode} ---\")\n",
        "            d = res[mode]\n",
        "            for key in [\"accuracy\",\"precision\",\"recall\",\"f1\",\"roc_auc\",\"confusion_matrix\"]:\n",
        "                print(key, \":\", d.get(key))\n",
        "    print_summary(\"WDBC\", res_wdbc)\n",
        "    print_summary(\"CreditApproval\", res_credit)\n",
        "\n",
        "    summary = []\n",
        "    for name, r in [(\"WDBC\",res_wdbc), (\"CreditApproval\",res_credit)]:\n",
        "        for mode in [\"scratch\",\"pretrained_ae\",\"pretrained_rbm\"]:\n",
        "            d = r[mode]\n",
        "            summary.append({\n",
        "                \"dataset\": name,\n",
        "                \"mode\": mode,\n",
        "                \"accuracy\": d[\"accuracy\"],\n",
        "                \"precision\": d[\"precision\"],\n",
        "                \"recall\": d[\"recall\"],\n",
        "                \"f1\": d[\"f1\"],\n",
        "                \"roc_auc\": d[\"roc_auc\"],\n",
        "                \"cm\": d[\"confusion_matrix\"].tolist()\n",
        "            })\n",
        "    df_summary = pd.DataFrame(summary)\n",
        "    df_summary.to_csv(\"pretrain_compare_summary_rbm.csv\", index=False)\n",
        "    print(\"\\nSaved summary to pretrain_compare_summary_rbm.csv\")\n"
      ]
    }
  ]
}