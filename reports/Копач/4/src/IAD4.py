import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import warnings

warnings.filterwarnings('ignore')

# ==================== –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• CTG –ò–ó –õ–†3 ====================
print("=" * 70)
print("–õ–ê–ë–û–†–ê–¢–û–†–ù–ê–Ø –†–ê–ë–û–¢–ê ‚Ññ4: –°–†–ê–í–ù–ï–ù–ò–ï –ú–ï–¢–û–î–û–í –ü–†–ï–î–û–ë–£–ß–ï–ù–ò–Ø")
print("–î–ê–ù–ù–´–ï: CTG (–ò–ó –õ–ê–ë–û–†–ê–¢–û–†–ù–û–ô –†–ê–ë–û–¢–´ ‚Ññ3)")
print("=" * 70)


def load_cardiotocography_data():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∫–∞—Ä–¥–∏–æ—Ç–æ–∫–æ–≥—Ä–∞—Ñ–∏–∏ –∏–∑ –õ–†3"""
    try:
        df = pd.read_excel('CTG.xls', sheet_name='Data', header=1)
        print("‚úÖ –î–∞–Ω–Ω—ã–µ CTG —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")

        # –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∫–∞–∫ –≤ –õ–†3
        df = df.dropna(axis=1, how='all')

        # –í—ã–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∫–∞–∫ –≤ –õ–†3
        feature_columns = ['LB', 'AC', 'FM', 'UC', 'DL', 'DS', 'DP',
                           'ASTV', 'MSTV', 'ALTV', 'MLTV',
                           'Width', 'Min', 'Max', 'Nmax', 'Nzeros',
                           'Mode', 'Mean', 'Median', 'Variance', 'Tendency']

        available_features = [col for col in feature_columns if col in df.columns]
        target_col = 'NSP' if 'NSP' in df.columns else 'CLASS'

        # –£–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        df_clean = df[available_features + [target_col]].dropna()

        X = df_clean[available_features]
        y = df_clean[target_col] - 1  # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ 0-based

        print(f"üìä –î–∞–Ω–Ω—ã–µ: {X.shape[0]} samples, {X.shape[1]} features")
        print(f"üéØ –ö–ª–∞—Å—Å—ã: {np.unique(y)}")
        print(f"üìà –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {np.bincount(y)}")

        return X, y, available_features

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
        return None, None, None


# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö CTG
X, y, feature_names = load_cardiotocography_data()

if X is None:
    print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ CTG")
    exit()

# ==================== –£–õ–£–ß–®–ï–ù–ù–ê–Ø RBM –†–ï–ê–õ–ò–ó–ê–¶–ò–Ø ====================
print("\n" + "=" * 50)
print("–£–õ–£–ß–®–ï–ù–ù–ê–Ø –†–ï–ê–õ–ò–ó–ê–¶–ò–Ø RBM")
print("=" * 50)


class ImprovedRBM(nn.Module):
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–∞—è –º–∞—à–∏–Ω–∞ –ë–æ–ª—å—Ü–º–∞–Ω–∞"""

    def __init__(self, n_visible, n_hidden):
        super(ImprovedRBM, self).__init__()
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ —Å –º–µ–Ω—å—à–µ–π –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π
        self.W = nn.Parameter(torch.randn(n_visible, n_hidden) * 0.01)
        self.v_bias = nn.Parameter(torch.zeros(n_visible))
        self.h_bias = nn.Parameter(torch.zeros(n_hidden))
        self.n_visible = n_visible
        self.n_hidden = n_hidden

    def sample_from_p(self, p):
        """–í—ã–±–æ—Ä–∫–∞ –∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ë–µ—Ä–Ω—É–ª–ª–∏"""
        return torch.bernoulli(p)

    def v_to_h(self, v):
        """–í–∏–¥–Ω—ã–µ -> —Å–∫—Ä—ã—Ç—ã–µ"""
        activation = torch.matmul(v, self.W) + self.h_bias
        p_h = torch.sigmoid(activation)
        return p_h, self.sample_from_p(p_h)

    def h_to_v(self, h):
        """–°–∫—Ä—ã—Ç—ã–µ -> –≤–∏–¥–Ω—ã–µ"""
        activation = torch.matmul(h, self.W.t()) + self.v_bias
        p_v = torch.sigmoid(activation)
        return p_v, self.sample_from_p(p_v)

    def contrastive_divergence(self, v0, k=1):
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω–æ–π –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏"""
        # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
        ph0, h0 = self.v_to_h(v0)

        # Gibbs sampling k —à–∞–≥–æ–≤
        vk = v0
        for _ in range(k):
            _, hk = self.v_to_h(vk)
            p_vk, vk = self.h_to_v(hk)
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à–æ–π —à—É–º –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            vk = vk + torch.randn_like(vk) * 0.01
            vk = torch.clamp(vk, 0, 1)

        # –û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥
        phk, _ = self.v_to_h(vk)

        return v0, vk, ph0, phk

    def free_energy(self, v):
        """–°–≤–æ–±–æ–¥–Ω–∞—è —ç–Ω–µ—Ä–≥–∏—è"""
        wx_b = torch.matmul(v, self.W) + self.h_bias
        vbias_term = torch.matmul(v, self.v_bias.unsqueeze(1)).squeeze()
        hidden_term = torch.sum(torch.log(1 + torch.exp(wx_b)), dim=1)
        return -hidden_term - vbias_term


class ImprovedRBMPretrainer:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è RBM"""

    def __init__(self, layer_dims):
        self.layer_dims = layer_dims
        self.rbms = []

    def pretrain_layer(self, X, n_visible, n_hidden, epochs=50, lr=0.01, k=1, momentum=0.9):
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ –æ–¥–Ω–æ–≥–æ —Å–ª–æ—è RBM"""
        print(f"üîß –û–±—É—á–µ–Ω–∏–µ RBM: {n_visible} ‚Üí {n_hidden}")

        rbm = ImprovedRBM(n_visible, n_hidden)

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ –±–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è RBM
        X_normalized = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0) + 1e-8)
        X_tensor = torch.FloatTensor(X_normalized)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–º–µ–Ω—Ç—É–º–∞
        W_momentum = torch.zeros_like(rbm.W)
        v_bias_momentum = torch.zeros_like(rbm.v_bias)
        h_bias_momentum = torch.zeros_like(rbm.h_bias)

        losses = []
        for epoch in range(epochs):
            epoch_loss = 0
            num_batches = 0

            # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∫–∞–∂–¥—ã–π —Ä–∞–∑
            indices = torch.randperm(len(X_tensor))

            for i in range(0, len(X_tensor), 32):
                batch_indices = indices[i:i + 32]
                batch = X_tensor[batch_indices]

                # Contrastive Divergence
                v0, vk, ph0, phk = rbm.contrastive_divergence(batch, k=k)

                # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
                positive_grad = torch.matmul(v0.t(), ph0)
                negative_grad = torch.matmul(vk.t(), phk)

                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å –º–æ–º–µ–Ω—Ç—É–º–æ–º
                delta_W = lr * ((positive_grad - negative_grad) / len(batch))
                delta_v_bias = lr * torch.mean(v0 - vk, dim=0)
                delta_h_bias = lr * torch.mean(ph0 - phk, dim=0)

                W_momentum = momentum * W_momentum + delta_W
                v_bias_momentum = momentum * v_bias_momentum + delta_v_bias
                h_bias_momentum = momentum * h_bias_momentum + delta_h_bias

                rbm.W.data += W_momentum
                rbm.v_bias.data += v_bias_momentum
                rbm.h_bias.data += h_bias_momentum

                # –ü–æ—Ç–µ—Ä–∏
                loss = torch.mean(rbm.free_energy(v0)) - torch.mean(rbm.free_energy(vk))
                epoch_loss += loss.item()
                num_batches += 1

            avg_loss = epoch_loss / num_batches if num_batches > 0 else 0
            losses.append(avg_loss)

            if (epoch + 1) % 10 == 0:
                print(f'   üìâ –≠–ø–æ—Ö–∞ [{epoch + 1}/{epochs}], –ü–æ—Ç–µ—Ä–∏: {avg_loss:.4f}')

        return rbm.W.data.clone(), rbm.h_bias.data.clone(), losses

    def pretrain_stack(self, X, epochs_per_layer=50):
        """–ü–æ—Å–ª–æ–π–Ω–æ–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ RBM"""
        print("üéØ –ù–∞—á–∞–ª–æ –ø–æ—Å–ª–æ–π–Ω–æ–≥–æ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è RBM...")
        current_data = X
        all_losses = []

        for i, n_hidden in enumerate(self.layer_dims):
            n_visible = current_data.shape[1]
            print(f"üìö –°–ª–æ–π {i + 1}: {n_visible} ‚Üí {n_hidden}")

            weights, biases, losses = self.pretrain_layer(current_data, n_visible, n_hidden, epochs_per_layer)
            self.rbms.append((weights, biases))
            all_losses.append(losses)

            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–ª–æ—è
            with torch.no_grad():
                # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–±—É—á–µ–Ω–Ω—É—é RBM –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–∫—Ä—ã—Ç—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π
                rbm_temp = ImprovedRBM(n_visible, n_hidden)
                rbm_temp.W.data = weights
                rbm_temp.h_bias.data = biases

                ph, _ = rbm_temp.v_to_h(torch.FloatTensor(current_data))
                current_data = ph.numpy()

        print("‚úÖ –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ RBM –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        return self.rbms, all_losses


# ==================== –ê–†–•–ò–¢–ï–ö–¢–£–†–´ –ú–û–î–ï–õ–ï–ô (–°–û–ì–õ–ê–°–û–í–ê–ù–ù–´–ï –° –õ–†3) ====================
class NeuralNetwork(nn.Module):
    """–ë–∞–∑–æ–≤–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å (—Ç–∞–∫–∞—è –∂–µ –∫–∞–∫ –≤ –õ–†3)"""

    def __init__(self, input_dim, num_classes):
        super(NeuralNetwork, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, num_classes)
        )

    def forward(self, x):
        return self.network(x)


class AutoencoderPretrainedNetwork(nn.Module):
    """–°–µ—Ç—å —Å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ–º –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–æ–º (—Ç–∞–∫–∞—è –∂–µ –∫–∞–∫ –≤ –õ–†3)"""

    def __init__(self, input_dim, num_classes, pretrained_weights):
        super(AutoencoderPretrainedNetwork, self).__init__()

        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–∏ —Å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏
        self.layer1 = nn.Linear(input_dim, 256)
        self.layer2 = nn.Linear(256, 128)
        self.layer3 = nn.Linear(128, 64)
        self.output_layer = nn.Linear(64, num_classes)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –∏–∑ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–æ–≤
        if len(pretrained_weights) >= 3:
            self.layer1.weight.data = pretrained_weights[0][0].clone()
            self.layer1.bias.data = pretrained_weights[0][1].clone()

            self.layer2.weight.data = pretrained_weights[1][0].clone()
            self.layer2.bias.data = pretrained_weights[1][1].clone()

            self.layer3.weight.data = pretrained_weights[2][0].clone()
            self.layer3.bias.data = pretrained_weights[2][1].clone()

        self.relu = nn.ReLU()
        self.dropout1 = nn.Dropout(0.4)
        self.dropout2 = nn.Dropout(0.3)
        self.dropout3 = nn.Dropout(0.2)

    def forward(self, x):
        x = self.relu(self.layer1(x))
        x = self.dropout1(x)
        x = self.relu(self.layer2(x))
        x = self.dropout2(x)
        x = self.relu(self.layer3(x))
        x = self.dropout3(x)
        x = self.output_layer(x)
        return x


class RBMPretrainedNetwork(nn.Module):
    """–°–µ—Ç—å —Å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ–º RBM"""

    def __init__(self, input_dim, num_classes, pretrained_weights):
        super(RBMPretrainedNetwork, self).__init__()

        # –¢–∞–∫–∞—è –∂–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∫–∞–∫ –≤ –õ–†3
        self.layer1 = nn.Linear(input_dim, 256)
        self.layer2 = nn.Linear(256, 128)
        self.layer3 = nn.Linear(128, 64)
        self.output_layer = nn.Linear(64, num_classes)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –∏–∑ RBM
        if len(pretrained_weights) >= 3:
            self.layer1.weight.data = pretrained_weights[0][0].t().clone()  # –¢—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä—É–µ–º
            self.layer1.bias.data = pretrained_weights[0][1].clone()

            self.layer2.weight.data = pretrained_weights[1][0].t().clone()
            self.layer2.bias.data = pretrained_weights[1][1].clone()

            self.layer3.weight.data = pretrained_weights[2][0].t().clone()
            self.layer3.bias.data = pretrained_weights[2][1].clone()

        self.relu = nn.ReLU()
        self.dropout1 = nn.Dropout(0.4)
        self.dropout2 = nn.Dropout(0.3)
        self.dropout3 = nn.Dropout(0.2)

    def forward(self, x):
        x = self.relu(self.layer1(x))
        x = self.dropout1(x)
        x = self.relu(self.layer2(x))
        x = self.dropout2(x)
        x = self.relu(self.layer3(x))
        x = self.dropout3(x)
        x = self.output_layer(x)
        return x


# ==================== –ê–í–¢–û–≠–ù–ö–û–î–ï–† –î–õ–Ø –ü–†–ï–î–û–ë–£–ß–ï–ù–ò–Ø (–ö–ê–ö –í –õ–†3) ====================
class AutoencoderPretrainer:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–æ–º (–∫–∞–∫ –≤ –õ–†3)"""

    def __init__(self, layer_dims):
        self.layer_dims = layer_dims
        self.autoencoders = []

    def pretrain_layer(self, X, input_dim, encoding_dim, epochs=50):
        """–ü—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ –æ–¥–Ω–æ–≥–æ —Å–ª–æ—è –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–æ–º"""
        print(f"üîß –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ —Å–ª–æ—è: {input_dim} ‚Üí {encoding_dim}")

        autoencoder = nn.Sequential(
            nn.Linear(input_dim, encoding_dim),
            nn.ReLU(),
            nn.Linear(encoding_dim, input_dim)
        )

        criterion = nn.MSELoss()
        optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)

        X_tensor = torch.FloatTensor(X)

        losses = []
        for epoch in range(epochs):
            autoencoder.train()
            total_loss = 0
            num_batches = 0

            for batch_idx in range(0, len(X_tensor), 32):
                batch = X_tensor[batch_idx:batch_idx + 32]
                optimizer.zero_grad()
                reconstructed = autoencoder(batch)
                loss = criterion(reconstructed, batch)
                loss.backward()
                optimizer.step()
                total_loss += loss.item()
                num_batches += 1

            avg_loss = total_loss / num_batches if num_batches > 0 else 0
            losses.append(avg_loss)

            if (epoch + 1) % 20 == 0:
                print(f'   üìâ –≠–ø–æ—Ö–∞ [{epoch + 1}/{epochs}], –ü–æ—Ç–µ—Ä–∏: {avg_loss:.4f}')

        return autoencoder[0].weight.data.clone(), autoencoder[0].bias.data.clone(), losses

    def pretrain_stack(self, X, epochs_per_layer=50):
        """–ü–æ—Å–ª–æ–π–Ω–æ–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ"""
        print("üîß –ù–∞—á–∞–ª–æ –ø–æ—Å–ª–æ–π–Ω–æ–≥–æ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–æ–º...")
        current_data = X
        all_losses = []

        for i, encoding_dim in enumerate(self.layer_dims):
            input_dim = current_data.shape[1]
            print(f"üìö –°–ª–æ–π {i + 1}: {input_dim} ‚Üí {encoding_dim}")

            weights, biases, losses = self.pretrain_layer(current_data, input_dim, encoding_dim, epochs_per_layer)
            self.autoencoders.append((weights, biases))
            all_losses.append(losses)

            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–ª–æ—è
            with torch.no_grad():
                linear_layer = nn.Linear(input_dim, encoding_dim)
                linear_layer.weight.data = weights
                linear_layer.bias.data = biases
                current_data = torch.relu(linear_layer(torch.FloatTensor(current_data))).numpy()

        print("‚úÖ –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–æ–º –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        return self.autoencoders, all_losses


# ==================== –§–£–ù–ö–¶–ò–Ø –û–ë–£–ß–ï–ù–ò–Ø ====================
def train_and_evaluate_model(model, train_loader, test_loader, epochs=100, model_name="–ú–æ–¥–µ–ª—å"):
    """–û–±—É—á–µ–Ω–∏–µ –∏ –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏"""
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    train_losses = []
    test_accuracies = []

    print(f"\nüéØ –û–±—É—á–µ–Ω–∏–µ {model_name}...")

    for epoch in range(epochs):
        # –û–±—É—á–µ–Ω–∏–µ
        model.train()
        total_loss = 0
        for batch_x, batch_y in train_loader:
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        model.eval()
        correct = 0
        total = 0
        all_preds = []
        all_labels = []

        with torch.no_grad():
            for batch_x, batch_y in test_loader:
                outputs = model(batch_x)
                _, predicted = torch.max(outputs.data, 1)
                total += batch_y.size(0)
                correct += (predicted == batch_y).sum().item()
                all_preds.extend(predicted.numpy())
                all_labels.extend(batch_y.numpy())

        accuracy = correct / total
        train_losses.append(total_loss / len(train_loader))
        test_accuracies.append(accuracy)

        if (epoch + 1) % 20 == 0:
            print(f'üìà –≠–ø–æ—Ö–∞ [{epoch + 1}/{epochs}], –ü–æ—Ç–µ—Ä–∏: {total_loss / len(train_loader):.4f}, '
                  f'–¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.4f}')

    # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
    final_accuracy = accuracy_score(all_labels, all_preds)
    final_f1 = f1_score(all_labels, all_preds, average='weighted')
    cm = confusion_matrix(all_labels, all_preds)

    print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã {model_name}:")
    print(f"   ‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å: {final_accuracy:.4f}")
    print(f"   ‚úÖ F1-score: {final_f1:.4f}")

    return final_accuracy, final_f1, cm, train_losses, test_accuracies


# ==================== –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–´ –ù–ê –î–ê–ù–ù–´–• CTG ====================
print("\n" + "=" * 70)
print("–≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢: –°–†–ê–í–ù–ï–ù–ò–ï –ú–ï–¢–û–î–û–í –ü–†–ï–î–û–ë–£–ß–ï–ù–ò–Ø –ù–ê CTG –î–ê–ù–ù–´–•")
print("=" * 70)

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö CTG (—Ç–∞–∫–∞—è –∂–µ –∫–∞–∫ –≤ –õ–†3)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

X_train_tensor = torch.FloatTensor(X_train_scaled)
y_train_tensor = torch.LongTensor(y_train.values)
X_test_tensor = torch.FloatTensor(X_test_scaled)
y_test_tensor = torch.LongTensor(y_test.values)

train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–µ–π (—Ç–∞–∫–∏–µ –∂–µ –∫–∞–∫ –≤ –õ–†3)
input_dim = X.shape[1]
num_classes = len(np.unique(y))
layer_dims = [256, 128, 64]  # –¢–∞–∫–∞—è –∂–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∫–∞–∫ –≤ –õ–†3
epochs = 100

print(f"üèóÔ∏è  –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–µ—Ç–∏: {input_dim} ‚Üí {layer_dims} ‚Üí {num_classes}")
print(f"üîÑ –≠–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è: {epochs}")
print(f"üìä –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_train_scaled.shape}")
print(f"üìä –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_test_scaled.shape}")

# 1. –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –±–µ–∑ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è
print("\n" + "=" * 50)
print("1. üöÄ –ë–ê–ó–û–í–ê–Ø –ú–û–î–ï–õ–¨ –ë–ï–ó –ü–†–ï–î–û–ë–£–ß–ï–ù–ò–Ø")
base_model = NeuralNetwork(input_dim, num_classes)
base_accuracy, base_f1, cm_base, base_train_losses, base_test_accuracies = train_and_evaluate_model(
    base_model, train_loader, test_loader, epochs=epochs,
    model_name="–ë–µ–∑ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è"
)

# 2. –ú–æ–¥–µ–ª—å —Å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ–º –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–æ–º
print("\n" + "=" * 50)
print("2. üéØ –ú–û–î–ï–õ–¨ –° –ü–†–ï–î–û–ë–£–ß–ï–ù–ò–ï–ú –ê–í–¢–û–≠–ù–ö–û–î–ï–†–û–ú")
ae_pretrainer = AutoencoderPretrainer(layer_dims)
ae_weights, ae_losses = ae_pretrainer.pretrain_stack(X_train_scaled, epochs_per_layer=50)

ae_model = AutoencoderPretrainedNetwork(input_dim, num_classes, ae_weights)
ae_accuracy, ae_f1, cm_ae, ae_train_losses, ae_test_accuracies = train_and_evaluate_model(
    ae_model, train_loader, test_loader, epochs=epochs,
    model_name="–° –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ–º (Autoencoder)"
)

# 3. –ú–æ–¥–µ–ª—å —Å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ–º RBM
print("\n" + "=" * 50)
print("3. üî• –ú–û–î–ï–õ–¨ –° –ü–†–ï–î–û–ë–£–ß–ï–ù–ò–ï–ú RBM")
rbm_pretrainer = ImprovedRBMPretrainer(layer_dims)
rbm_weights, rbm_losses = rbm_pretrainer.pretrain_stack(X_train_scaled, epochs_per_layer=50)

rbm_model = RBMPretrainedNetwork(input_dim, num_classes, rbm_weights)
rbm_accuracy, rbm_f1, cm_rbm, rbm_train_losses, rbm_test_accuracies = train_and_evaluate_model(
    rbm_model, train_loader, test_loader, epochs=epochs,
    model_name="–° –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ–º (RBM)"
)

# ==================== –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ====================
print("\n" + "=" * 70)
print("–í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –°–†–ê–í–ù–ï–ù–ò–Ø")
print("=" * 70)

# –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
fig = plt.figure(figsize=(20, 15))

# 1. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ F1-score
ax1 = plt.subplot2grid((3, 3), (0, 0), colspan=2)
methods = ['–ë–µ–∑ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è', 'Autoencoder', 'RBM']
accuracies = [base_accuracy, ae_accuracy, rbm_accuracy]
f1_scores = [base_f1, ae_f1, rbm_f1]

x = np.arange(len(methods))
width = 0.35

bars1 = ax1.bar(x - width / 2, accuracies, width, label='–¢–æ—á–Ω–æ—Å—Ç—å', alpha=0.8, color='skyblue')
bars2 = ax1.bar(x + width / 2, f1_scores, width, label='F1-score', alpha=0.8, color='lightcoral')

ax1.set_title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –º–æ–¥–µ–ª–µ–π\n(–î–∞–Ω–Ω—ã–µ CTG)', fontweight='bold', fontsize=14)
ax1.set_ylabel('Score')
ax1.set_xticks(x)
ax1.set_xticklabels(methods, rotation=45)
ax1.legend()
ax1.grid(True, alpha=0.3)

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width() / 2., height + 0.01,
                 f'{height:.3f}', ha='center', va='bottom', fontweight='bold')

# 2. –ú–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫
class_names = ['–ù–æ—Ä–º–∞–ª—å–Ω—ã–π', '–ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–π', '–ü–∞—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π']

ax2 = plt.subplot2grid((3, 3), (0, 2))
sns.heatmap(cm_base, annot=True, fmt='d', cmap='Blues', ax=ax2,
            xticklabels=class_names, yticklabels=class_names)
ax2.set_title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫\n(–ë–µ–∑ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è)', fontweight='bold')
ax2.set_xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')
ax2.set_ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å')

ax3 = plt.subplot2grid((3, 3), (1, 0))
sns.heatmap(cm_ae, annot=True, fmt='d', cmap='Blues', ax=ax3,
            xticklabels=class_names, yticklabels=class_names)
ax3.set_title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫\n(Autoencoder)', fontweight='bold')
ax3.set_xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')
ax3.set_ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å')

ax4 = plt.subplot2grid((3, 3), (1, 1))
sns.heatmap(cm_rbm, annot=True, fmt='d', cmap='Blues', ax=ax4,
            xticklabels=class_names, yticklabels=class_names)
ax4.set_title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫\n(RBM)', fontweight='bold')
ax4.set_xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')
ax4.set_ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å')

# 3. –ö—Ä–∏–≤—ã–µ –æ–±—É—á–µ–Ω–∏—è
ax5 = plt.subplot2grid((3, 3), (1, 2))
ax5.plot(base_test_accuracies, label='–ë–µ–∑ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è', linewidth=2)
ax5.plot(ae_test_accuracies, label='Autoencoder', linewidth=2)
ax5.plot(rbm_test_accuracies, label='RBM', linewidth=2)
ax5.set_title('–¢–æ—á–Ω–æ—Å—Ç—å –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è', fontweight='bold')
ax5.set_xlabel('–≠–ø–æ—Ö–∞')
ax5.set_ylabel('–¢–æ—á–Ω–æ—Å—Ç—å')
ax5.legend()
ax5.grid(True, alpha=0.3)

# 4. –ü–æ—Ç–µ—Ä–∏ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è
ax6 = plt.subplot2grid((3, 3), (2, 0))
ax6.plot(base_train_losses, label='–ë–µ–∑ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è', linewidth=2)
ax6.plot(ae_train_losses, label='Autoencoder', linewidth=2)
ax6.plot(rbm_train_losses, label='RBM', linewidth=2)
ax6.set_title('–ü–æ—Ç–µ—Ä–∏ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è', fontweight='bold')
ax6.set_xlabel('–≠–ø–æ—Ö–∞')
ax6.set_ylabel('–ü–æ—Ç–µ—Ä–∏')
ax6.legend()
ax6.grid(True, alpha=0.3)

# 5. –ü–æ—Ç–µ—Ä–∏ –ø—Ä–∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–∏
ax7 = plt.subplot2grid((3, 3), (2, 1))
for i, losses in enumerate(ae_losses):
    ax7.plot(losses, label=f'AE –°–ª–æ–π {i + 1}', linestyle='--')
for i, losses in enumerate(rbm_losses):
    ax7.plot(losses, label=f'RBM –°–ª–æ–π {i + 1}', linestyle='-')
ax7.set_title('–ü–æ—Ç–µ—Ä–∏ –ø—Ä–∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–∏', fontweight='bold')
ax7.set_xlabel('–≠–ø–æ—Ö–∞')
ax7.set_ylabel('–ü–æ—Ç–µ—Ä–∏')
ax7.legend()
ax7.grid(True, alpha=0.3)

# 6. –ê–Ω–∞–ª–∏–∑ —É–ª—É—á—à–µ–Ω–∏–π
ax8 = plt.subplot2grid((3, 3), (2, 2))
improvements_acc = [ae_accuracy - base_accuracy, rbm_accuracy - base_accuracy]
improvements_f1 = [ae_f1 - base_f1, rbm_f1 - base_f1]

x_imp = np.arange(2)
width_imp = 0.35

bars_imp1 = ax8.bar(x_imp - width_imp / 2, improvements_acc, width_imp,
                    label='–£–ª—É—á—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏', alpha=0.8,
                    color=['green' if x > 0 else 'red' for x in improvements_acc])
bars_imp2 = ax8.bar(x_imp + width_imp / 2, improvements_f1, width_imp,
                    label='–£–ª—É—á—à–µ–Ω–∏–µ F1', alpha=0.8,
                    color=['green' if x > 0 else 'red' for x in improvements_f1])

ax8.set_title('–£–ª—É—á—à–µ–Ω–∏–µ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏', fontweight='bold')
ax8.set_ylabel('–£–ª—É—á—à–µ–Ω–∏–µ')
ax8.set_xticks(x_imp)
ax8.set_xticklabels(['Autoencoder', 'RBM'])
ax8.axhline(y=0, color='black', linestyle='-', alpha=0.3)
ax8.legend()
ax8.grid(True, alpha=0.3)

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π
for bars in [bars_imp1, bars_imp2]:
    for bar in bars:
        height = bar.get_height()
        ax8.text(bar.get_x() + bar.get_width() / 2., height + (0.001 if height >= 0 else -0.01),
                 f'{height:+.3f}', ha='center', va='bottom' if height >= 0 else 'top',
                 fontweight='bold', color='black')

plt.tight_layout()
plt.show()

# ==================== –§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ ====================
print("\n" + "=" * 70)
print("–§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢: –°–†–ê–í–ù–ï–ù–ò–ï –ú–ï–¢–û–î–û–í –ü–†–ï–î–û–ë–£–ß–ï–ù–ò–Ø")
print("=" * 70)

print(f"\nüìä –°–†–ê–í–ù–ò–¢–ï–õ–¨–ù–ê–Ø –¢–ê–ë–õ–ò–¶–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í:")
print(f"{'–ú–µ—Ç–æ–¥':<25} {'–¢–æ—á–Ω–æ—Å—Ç—å':<12} {'F1-Score':<12} {'–£–ª—É—á—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏':<18}")
print(f"{'-' * 70}")
print(f"{'–ë–µ–∑ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è':<25} {base_accuracy:.4f}     {base_f1:.4f}     {'-':<18}")
print(f"{'Autoencoder':<25} {ae_accuracy:.4f}     {ae_f1:.4f}     {ae_accuracy - base_accuracy:+.4f}")
print(f"{'RBM':<25} {rbm_accuracy:.4f}     {rbm_f1:.4f}     {rbm_accuracy - base_accuracy:+.4f}")

print(f"\nüîß –ü–ê–†–ê–ú–ï–¢–†–´ –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–ê:")
print(f"   ‚Ä¢ –î–∞–Ω–Ω—ã–µ: CTG ({X.shape[0]} samples, {X.shape[1]} features)")
print(f"   ‚Ä¢ –ö–ª–∞—Å—Å—ã: {num_classes} ({class_names})")
print(f"   ‚Ä¢ –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: {input_dim} ‚Üí {layer_dims} ‚Üí {num_classes}")
print(f"   ‚Ä¢ –≠–ø–æ—Ö –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è: 50 –Ω–∞ —Å–ª–æ–π")
print(f"   ‚Ä¢ –≠–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è: {epochs}")

print(f"\nüìà –ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í:")
best_method = np.argmax([base_accuracy, ae_accuracy, rbm_accuracy])
best_method_name = methods[best_method]
best_accuracy = [base_accuracy, ae_accuracy, rbm_accuracy][best_method]

print(f"   ‚Ä¢ –õ—É—á—à–∏–π –º–µ—Ç–æ–¥: {best_method_name} (—Ç–æ—á–Ω–æ—Å—Ç—å: {best_accuracy:.4f})")

if ae_accuracy > base_accuracy and rbm_accuracy > base_accuracy:
    print(f"   ‚Ä¢ –û–±–∞ –º–µ—Ç–æ–¥–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è —É–ª—É—á—à–∏–ª–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å")
elif ae_accuracy > base_accuracy:
    print(f"   ‚Ä¢ –¢–æ–ª—å–∫–æ Autoencoder —É–ª—É—á—à–∏–ª –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å")
elif rbm_accuracy > base_accuracy:
    print(f"   ‚Ä¢ –¢–æ–ª—å–∫–æ RBM —É–ª—É—á—à–∏–ª –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å")
else:
    print(f"   ‚Ä¢ –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ –Ω–µ –¥–∞–ª–æ —É–ª—É—á—à–µ–Ω–∏—è")

print(f"\nüéØ –í–´–í–û–î–´:")
print(f"   ‚Ä¢ RBM –ø–æ–∫–∞–∑–∞–ª {'–ª—É—á—à–∏–µ' if rbm_accuracy > ae_accuracy else '—Ö—É–¥—à–∏–µ'} —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —á–µ–º Autoencoder")
print(f"   ‚Ä¢ –†–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É –º–µ—Ç–æ–¥–∞–º–∏: {abs(rbm_accuracy - ae_accuracy):.4f}")

print(f"\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
if best_method == 0:
    print("   ‚Ä¢ –î–ª—è –¥–∞–Ω–Ω—ã—Ö CTG –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –æ–±—É—á–µ–Ω–∏—è –±–µ–∑ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è")
elif best_method == 1:
    print("   ‚Ä¢ –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Autoencoder –¥–ª—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è")
else:
    print("   ‚Ä¢ –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å RBM –¥–ª—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è")

print("\n" + "=" * 70)
print("–õ–ê–ë–û–†–ê–¢–û–†–ù–ê–Ø –†–ê–ë–û–¢–ê ‚Ññ4 –ó–ê–í–ï–†–®–ï–ù–ê! üéâ")
print("=" * 70)