import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.metrics import f1_score, classification_report, confusion_matrix, precision_recall_curve, auc
from sklearn.utils import class_weight
from catboost import CatBoostClassifier
from xgboost import XGBClassifier
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

warnings.filterwarnings('ignore')

print("=" * 60)
print("–ü–ï–†–ï–ó–ê–ü–£–°–ö –ê–ù–ê–õ–ò–ó–ê –° –£–ß–ï–¢–û–ú –†–ê–ó–ú–ï–†–ê –î–ê–¢–ê–°–ï–¢–ê")
print("=" * 60)

# 1. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö
df = pd.read_csv('Telco-Customer-Churn.csv')
print(f"–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {df.shape[0]} —Å—Ç—Ä–æ–∫, {df.shape[1]} —Å—Ç–æ–ª–±—Ü–æ–≤")
print(f"–û–±—ä–µ–º –¥–∞–Ω–Ω—ã—Ö: {df.shape[0] - 1} –∫–ª–∏–µ–Ω—Ç–æ–≤ + 1 –∑–∞–≥–æ–ª–æ–≤–æ–∫")

# 2. –ë–æ–ª–µ–µ —Ç—â–∞—Ç–µ–ª—å–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
# –û–±—Ä–∞–±–æ—Ç–∫–∞ TotalCharges
df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')
print(f"–ü—Ä–æ–ø—É—Å–∫–∏ –≤ TotalCharges: {df['TotalCharges'].isnull().sum()}")

# –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–ø—É—Å–∫–æ–≤
missing_data = df[df['TotalCharges'].isnull()]
print(f"–ö–ª–∏–µ–Ω—Ç—ã —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏ –≤ TotalCharges:")
print(f"  - Tenure: {missing_data['tenure'].unique()}")
print(f"  - MonthlyCharges: {missing_data['MonthlyCharges'].unique()}")

# –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ 0 (–Ω–æ–≤—ã–µ –∫–ª–∏–µ–Ω—Ç—ã)
df['TotalCharges'].fillna(0, inplace=True)

# –£–¥–∞–ª—è–µ–º customerID
df = df.drop('customerID', axis=1)

# 3. –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π feature engineering
# –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
df['TenureGroup'] = pd.cut(df['tenure'],
                           bins=[0, 12, 24, 36, 48, 60, 72],
                           labels=['0-1y', '1-2y', '2-3y', '3-4y', '4-5y', '5-6y'])

df['ChargeRatio'] = df['MonthlyCharges'] / (df['TotalCharges'] / (df['tenure'] + 1))
df['ChargeRatio'].replace([np.inf, -np.inf], 0, inplace=True)
df['ChargeRatio'].fillna(0, inplace=True)

df['AvgMonthlyCharge'] = df['TotalCharges'] / (df['tenure'] + 1)

# 4. –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
le_target = LabelEncoder()
df['Churn'] = le_target.fit_transform(df['Churn'])

# –†–∞–∑–¥–µ–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
numeric_features = ['SeniorCitizen', 'tenure', 'MonthlyCharges', 'TotalCharges',
                    'ChargeRatio', 'AvgMonthlyCharge']
categorical_features = [col for col in df.columns if col not in numeric_features + ['Churn', 'TenureGroup']]

# –ö–æ–¥–∏—Ä—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
label_encoders = {}
for col in categorical_features:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col].astype(str))
    label_encoders[col] = le

# –ö–æ–¥–∏—Ä—É–µ–º TenureGroup
df['TenureGroup'] = LabelEncoder().fit_transform(df['TenureGroup'])

# 5. –ê–Ω–∞–ª–∏–∑ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤
print(f"\n–†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ö–õ–ê–°–°–û–í:")
churn_counts = df['Churn'].value_counts()
print(f"–ù–µ –æ—Ç—Ç–æ–∫ (0): {churn_counts[0]} ({churn_counts[0] / len(df) * 100:.1f}%)")
print(f"–û—Ç—Ç–æ–∫ (1): {churn_counts[1]} ({churn_counts[1] / len(df) * 100:.1f}%)")

# 6. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
X = df.drop('Churn', axis=1)
y = df['Churn']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

print(f"\n–†–ê–ó–î–ï–õ–ï–ù–ò–ï –î–ê–ù–ù–´–•:")
print(f"–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_train.shape[0]} –∫–ª–∏–µ–Ω—Ç–æ–≤")
print(f"–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_test.shape[0]} –∫–ª–∏–µ–Ω—Ç–æ–≤")

# 7. –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
scaler = StandardScaler()
X_train_scaled = X_train.copy()
X_test_scaled = X_test.copy()

X_train_scaled[numeric_features] = scaler.fit_transform(X_train[numeric_features])
X_test_scaled[numeric_features] = scaler.transform(X_test[numeric_features])

# 8. –†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
models = {
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Random Forest': RandomForestClassifier(random_state=42),
    'AdaBoost': AdaBoostClassifier(random_state=42),
    'XGBoost': XGBClassifier(random_state=42, eval_metric='logloss'),
    'CatBoost': CatBoostClassifier(random_state=42, verbose=False)
}

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è GridSearch
param_grids = {
    'Decision Tree': {
        'max_depth': [3, 5, 7, 10],
        'min_samples_split': [2, 5, 10]
    },
    'Random Forest': {
        'n_estimators': [100, 200],
        'max_depth': [10, 20],
        'min_samples_split': [2, 5]
    },
    'AdaBoost': {
        'n_estimators': [50, 100],
        'learning_rate': [0.1, 0.5, 1.0]
    }
}

results = {}
best_models = {}

print("\n" + "=" * 50)
print("–†–ê–°–®–ò–†–ï–ù–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô")
print("=" * 50)

for name, model in models.items():
    print(f"\n--- {name} ---")

    if name in param_grids:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º GridSearch –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        grid_search = GridSearchCV(model, param_grids[name], cv=5, scoring='f1', n_jobs=-1)

        if name in ['XGBoost', 'CatBoost']:
            grid_search.fit(X_train, y_train)
            best_model = grid_search.best_estimator_
            y_pred = best_model.predict(X_test)
            y_pred_proba = best_model.predict_proba(X_test)[:, 1]
        else:
            grid_search.fit(X_train_scaled, y_train)
            best_model = grid_search.best_estimator_
            y_pred = best_model.predict(X_test_scaled)
            y_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]

        print(f"  –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {grid_search.best_params_}")

    else:
        # –ë–µ–∑ GridSearch
        if name in ['XGBoost', 'CatBoost']:
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            y_pred_proba = model.predict_proba(X_test)[:, 1]
            best_model = model
        else:
            model.fit(X_train_scaled, y_train)
            y_pred = model.predict(X_test_scaled)
            y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
            best_model = model

    f1 = f1_score(y_test, y_pred)
    results[name] = {
        'f1_score': f1,
        'model': best_model,
        'y_pred_proba': y_pred_proba
    }

    print(f"  F1-score: {f1:.4f}")

# 9. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
print("\n" + "=" * 50)
print("–ò–¢–û–ì–û–í–û–ï –°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô")
print("=" * 50)

sorted_results = sorted(results.items(), key=lambda x: x[1]['f1_score'], reverse=True)

print("\n–§–ò–ù–ê–õ–¨–ù–´–ô –†–ï–ô–¢–ò–ù–ì:")
for i, (name, result) in enumerate(sorted_results, 1):
    print(f"{i}. {name}: F1-score = {result['f1_score']:.4f}")

# 10. –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
best_model_name, best_result = sorted_results[0]
print(f"\n–î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –õ–£–ß–®–ï–ô –ú–û–î–ï–õ–ò: {best_model_name}")

if best_model_name in ['XGBoost', 'CatBoost']:
    y_pred_best = best_result['model'].predict(X_test)
    print(classification_report(y_test, y_pred_best))
else:
    y_pred_best = best_result['model'].predict(X_test_scaled)
    print(classification_report(y_test, y_pred_best))

# 11. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
plt.figure(figsize=(15, 10))

# –ì—Ä–∞—Ñ–∏–∫ 1: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ F1-score
plt.subplot(2, 3, 1)
model_names = list(results.keys())
f1_scores = [result['f1_score'] for result in results.values()]

bars = plt.bar(model_names, f1_scores, color=['skyblue', 'lightcoral', 'lightgreen', 'gold', 'lightpink'])
plt.title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ F1-score –º–æ–¥–µ–ª–µ–π\n(7043 –∫–ª–∏–µ–Ω—Ç–∞)', fontsize=12, fontweight='bold')
plt.ylabel('F1-score')
plt.xticks(rotation=45)
plt.ylim(0, 0.7)

for bar, score in zip(bars, f1_scores):
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,
             f'{score:.4f}', ha='center', va='bottom', fontweight='bold')

# –ì—Ä–∞—Ñ–∏–∫ 2: –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
plt.subplot(2, 3, 2)
if hasattr(best_result['model'], 'feature_importances_'):
    feature_importance = best_result['model'].feature_importances_
    feature_importance_df = pd.DataFrame({
        'feature': X.columns,
        'importance': feature_importance
    }).sort_values('importance', ascending=False).head(10)

    sns.barplot(data=feature_importance_df, x='importance', y='feature', palette='viridis')
    plt.title(f'–í–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ ({best_model_name})', fontsize=12, fontweight='bold')

# –ì—Ä–∞—Ñ–∏–∫ 3: –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
plt.subplot(2, 3, 3)
cm = confusion_matrix(y_test, y_pred_best)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏', fontsize=12, fontweight='bold')
plt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ')
plt.ylabel('–§–∞–∫—Ç–∏—á–µ—Å–∫–∏')

# –ì—Ä–∞—Ñ–∏–∫ 4: Precision-Recall curve
plt.subplot(2, 3, 4)
for name, result in results.items():
    precision, recall, _ = precision_recall_curve(y_test, result['y_pred_proba'])
    pr_auc = auc(recall, precision)
    plt.plot(recall, precision, label=f'{name} (AUC={pr_auc:.3f})')

plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall –∫—Ä–∏–≤—ã–µ', fontsize=12, fontweight='bold')
plt.legend()

# –ì—Ä–∞—Ñ–∏–∫ 5: –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Ç—Ç–æ–∫–∞
plt.subplot(2, 3, 5)
churn_dist = df['Churn'].value_counts()
plt.pie(churn_dist, labels=['–ù–µ –æ—Ç—Ç–æ–∫', '–û—Ç—Ç–æ–∫'], autopct='%1.1f%%',
        colors=['lightblue', 'lightcoral'], startangle=90)
plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Ç—Ç–æ–∫–∞ –∫–ª–∏–µ–Ω—Ç–æ–≤', fontsize=12, fontweight='bold')

# –ì—Ä–∞—Ñ–∏–∫ 6: –¢–æ–ø-5 –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—â–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
plt.subplot(2, 3, 6)
correlations = df.corr()['Churn'].drop('Churn').sort_values(ascending=False).head(5)
sns.barplot(x=correlations.values, y=correlations.index, palette='coolwarm')
plt.title('–¢–æ–ø-5 –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π —Å –æ—Ç—Ç–æ–∫–æ–º', fontsize=12, fontweight='bold')
plt.xlabel('–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è')

plt.tight_layout()
plt.show()

# 12. –ë–∏–∑–Ω–µ—Å-—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ 7043 –∫–ª–∏–µ–Ω—Ç–æ–≤
print("\n" + "=" * 60)
print("–ë–ò–ó–ù–ï–°-–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ù–ê –û–°–ù–û–í–ï –ê–ù–ê–õ–ò–ó–ê 7043 –ö–õ–ò–ï–ù–¢–û–í")
print("=" * 60)

total_customers = len(df)
churn_rate = churn_counts[1] / total_customers * 100

print(f"üìä –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
print(f"   ‚Ä¢ –í—Å–µ–≥–æ –∫–ª–∏–µ–Ω—Ç–æ–≤: {total_customers}")
print(f"   ‚Ä¢ –£—Ä–æ–≤–µ–Ω—å –æ—Ç—Ç–æ–∫–∞: {churn_rate:.1f}%")
print(f"   ‚Ä¢ –ö–ª–∏–µ–Ω—Ç–æ–≤ —Å —Ä–∏—Å–∫–æ–º –æ—Ç—Ç–æ–∫–∞: {churn_counts[1]}")

print(f"\nüéØ –≠–§–§–ï–ö–¢–ò–í–ù–û–°–¢–¨ –ú–û–î–ï–õ–ï–ô:")
print(f"   ‚Ä¢ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model_name} (F1-score: {best_result['f1_score']:.4f})")
print(f"   ‚Ä¢ –ú–æ–∂–µ—Ç –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å ~{best_result['f1_score'] * 100:.1f}% –æ—Ç—Ç–æ–∫–æ–≤")

print(f"\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
print(f"   ‚úì –§–æ–∫—É—Å –Ω–∞ {best_model_name} –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ç—Ç–æ–∫–∞")
print(f"   ‚úì –í–Ω–µ–¥—Ä–∏—Ç—å —Å–∏—Å—Ç–µ–º—É —Ä–∞–Ω–Ω–µ–≥–æ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è")
print(f"   ‚úì –ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —É–¥–µ—Ä–∂–∞–Ω–∏–µ –¥–ª—è {churn_counts[1]} –∫–ª–∏–µ–Ω—Ç–æ–≤ –≥—Ä—É–ø–ø—ã —Ä–∏—Å–∫–∞")
print(f"   ‚úì –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å –∫–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (—Ç–µ–Ω—É—Ä–∞, —Ç–∏–ø –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞, –ø–ª–∞—Ç–µ–∂–∏)")
print(f"   ‚úì A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π —É–¥–µ—Ä–∂–∞–Ω–∏—è")

print(f"\nüìà –ü–û–¢–ï–ù–¶–ò–ê–õ–¨–ù–´–ô –≠–§–§–ï–ö–¢:")
potential_savings = churn_counts[1] * 50  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å –ø—Ä–∏–≤–ª–µ—á–µ–Ω–∏—è –Ω–æ–≤–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞
print(f"   ‚Ä¢ –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è —ç–∫–æ–Ω–æ–º–∏—è: ${potential_savings:,.0f} (–ø—Ä–∏ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –ø—Ä–∏–≤–ª–µ—á–µ–Ω–∏—è $50/–∫–ª–∏–µ–Ω—Ç)")

# 13. –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –¥–ª—è –±–∏–∑–Ω–µ—Å-–∫–µ–π—Å–æ–≤
print(f"\n" + "=" * 50)
print("–ü–†–ê–ö–¢–ò–ß–ï–°–ö–û–ï –ü–†–ò–ú–ï–ù–ï–ù–ò–ï")
print("=" * 50)

# –ö–ª–∏–µ–Ω—Ç—ã —Å –≤—ã—Å–æ–∫–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é –æ—Ç—Ç–æ–∫–∞
high_risk_threshold = 0.7
high_risk_indices = np.where(best_result['y_pred_proba'] > high_risk_threshold)[0]

print(f"–ö–ª–∏–µ–Ω—Ç—ã —Å –≤—ã—Å–æ–∫–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é –æ—Ç—Ç–æ–∫–∞ (>70%): {len(high_risk_indices)}")
print(f"–ò–∑ –Ω–∏—Ö –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —É–π–¥—É—Ç: {sum(y_test.iloc[high_risk_indices] == 1)}")

# –ü—Ä–∏–º–µ—Ä –ø—Ä–æ–≥–Ω–æ–∑–æ–≤
sample_predictions = pd.DataFrame({
    '–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å_–æ—Ç—Ç–æ–∫–∞': best_result['y_pred_proba'][:10],
    '–ü—Ä–æ–≥–Ω–æ–∑': ['–í—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫' if x > 0.7 else '–°—Ä–µ–¥–Ω–∏–π —Ä–∏—Å–∫' if x > 0.3 else '–ù–∏–∑–∫–∏–π —Ä–∏—Å–∫' for x in
                best_result['y_pred_proba'][:10]],
    '–§–∞–∫—Ç–∏—á–µ—Å–∫–∏': ['–û—Ç—Ç–æ–∫' if x == 1 else '–ù–µ –æ—Ç—Ç–æ–∫' for x in y_test.iloc[:10]]
})

print(f"\n–ü—Ä–∏–º–µ—Ä –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –¥–ª—è –ø–µ—Ä–≤—ã—Ö 10 –∫–ª–∏–µ–Ω—Ç–æ–≤:")
print(sample_predictions)